
\section{Benchmarks and Workloads}
\label{sec:methodology:benchmarks}

In this section, we will present the benchmarks we use to evaluate cache partitioning algorithms in this thesis.
We explain how the we extracted simulation traces from benchmarks and how we classified those traces based on their sensitivity to changes in available cache space and memory bandwidth. 
Finally, we explain how we created 4-, 8-, and 16-core workloads based on those benchmark traces.

\subsection{Benchmarks and Sample Extraction}
\label{sec:methodology:benchmarks:benchmarks}

In all our experiments, we are utilizing benchmarks from the SPEC CPU2006~\cite{SPECCPU2006} benchmark suite. 
We choose this suite because it is the newest of the CPU benchmark suites from SPEC, and it is specifically designed to test the performance of various computer architectures with benchmarks based on real user applications.
Unless otherwise stated all benchmarks use the first reference input set.
Because simulating an entire benchmark is a time-consuming process, we choose to extract a sample interval used to represent each benchmark.
There are multiple ways of extracting sample intervals.
The naive way of choosing intervals is to specify an offset and a length and use this across all benchmarks.
However, several more advanced methods of sample selections exists, two examples are SimPoint~\cite{Hamerly2005} and SimFlex/SMARTS~\cite{Hardavellas2004, Wunderlich2003}.

SimPoint analyses the benchmark and divides it into basic blocks.
It then divides the dynamic instruction stream of the running benchmark into intervals of a set size.
Each interval is classified by which of the basics blocks in the benchmark the interval executes.
Finally, the intervals are clustered using k-means, and the algorithm selects the intervals closest to the centroid in each of the k-clusters to represent the benchmark.
The value of k is configurable.
SimPoint is run ahead of time and after it has been run an external simulator is used to simulate each of the SimPoint selected intervals.

SimFlex also divides the dynamic instruction stream into intervals, but unlike SimPoint this is done while the simulator is simulating the benchmark.
Some intervals are simulated in detail while others are simulated in a low detail fast forward mode.
SimFlex uses statistics to calculate the variation of simulation results based on the intervals simulated in detail.
The simulation ends after a minimum number of intervals has been simulated, and the result variation between the intervals is within a set limit.

Both solutions utilize the same idea of simulation only parts of the dynamic instruction stream.
The main difference is the selection criteria and the separation between ahead of time processing and integration with the simulator.
J. Yi et al.~\cite{Yi2005} has shown that there little is variation in accuracy between SimPoint and SimFlex selected intervals.
SimPoint also integrates well with our existing simulation framework.
As a result, we chose to utilize SimPoint in our work.

Ideally a few sample intervals are needed per benchmark to get accurate simulation results.
To simplify the simulation, we chose to extract one larger interval per benchmark, in place of multiple smaller ones.
By doing this, we are willingly increasing the error~\cite{Hamerly2004} between simulating our interval and the results obtained by simulating the entire benchmark.
In our experiments, we are interested in observing performance change to our simulated intervals due to architectural changes.
Producing results comparable to the results of a full benchmark run is not required to achieve this.
Also, it is not obvious how to correctly combine performance metrics from multiple simulated intervals.
Therefore, we choose to use only one interval per benchmark.

The length chosen for our intervals may also affect the final results.
As caches are empty when the simulation starts the cold cache effect, caused by compulsory misses at simulation start, may skew our results if the simulated interval is short.
Additionally, as we are experimenting with cache partitioning algorithms, we believe that a certain number of instructions are needed for our results to stabilize.
Finally, by increasing the number of instructions we are also increasing the time required to simulate a benchmark.
We choose to extract 250M instruction intervals using SimPoint.
This number of instructions will make the cold cache effect neglishable~\cite{Hamerly2005,Hamerly2004,Olsen2014} while we keep the simulation time relatively low.
We generate an instruction trace using Sniper for each SimPoint extracted interval.
All later experiments utilize these traces in place of the actual benchmark executable.

\subsection{Benchmark Classification}
\label{sec:methodology:benchmarks:classification}
\input{figures/workloads/benchmarks.tex}

To better understand our simulation results, we perform a benchmark classification experiment on each of the previously generated traces.
This experiment is intended to detect various properties in each trace that may affect how they behave on our simulated architecture with various cache partitioning algorithms.
We choose to categorize traces based on their sensitivity to the size of the LLC and the bandwidth of the bus connecting the LLC and the DRAM.

The system model used in this experiment is as shown in Table~\ref{tbl:processor_model:properties} with the smallest of the L2 configurations, 128KB.
The size of the L3 cache and the speed of the memory bus is varied as shown in Table~\ref{tbl:benchmarks_workloads:classification_model}.
By reducing the size of the L3 cache and the speed of the memory bus, we intend to simulate a situation where the benchmark has reduced access to resources due to contention.

\begin{table}[ht]
\centering
\begin{tabular}{rl}
\toprule
\bf{L3 Cache size} & 256kB, 512kB, 1024kB, 2048kB, 4096kB \\
\bf{Memory Bus Speed}   & 1.6GB/s, 3.2GB/s, 6.4GB/s, 12.8GB/s \\
\bottomrule                             
\end{tabular}
\caption{Model properties.}
\label{tbl:benchmarks_workloads:classification_model}
\end{table}

We simulate each benchmark for each combination of LLC size and memory bus bandwidth, in total 20 simulations per benchmark.
Then we evaluate how changes to the architecture affected the benchmarks performance using the reported IPC.
The evaluation is done by organizing the IPC measurements in a 2d data table with LLC size on one axis and bandwidth on the other.
Between each data pair along each axis, we calculate the performance reduction.
Using the arithmetic average of reductions along each axis, we classify each benchmark as either sensitive or not sensitive to changes in that axis property.
The standard deviation is used to increase the required limits on the average reduction if a benchmark shows high variability along an axis.
We observed this in cases where there is little change in performance except for one point where performance is significantly worse, dragging the average value down.
We define four groups for our benchmarks as described below.
For each group, we define rules used to detect benchmarks in that group.
\begin{itemize}

\item \textbf{Cache sensitive} (ca) benchmarks are in general benchmarks with memory access patterns that are recency-friendly. We required an average performance reduction along the cache axis of at least 4\% and a standard deviation of less than 11\% to classify a benchmark as cache sensitive. If the deviation is higher than 11\%, a performance reduction of at least 13\% is required to be classified as cache sensitive.

\item \textbf{Bandwidth sensitive} (bw) are benchmarks with no to little temporal locality, often streaming access patterns. We required an average performance reduction along the bandwidth axis of at least 8\% and a standard deviation of less than 11\% to classify a benchmark as bandwidth sensitive. If the deviation is higher than 11\%, an average performance reduction of at least 20\% is required to be classified as bandwidth sensitive. 

\item \textbf{Cache- and Bandwidth sensitive} (cabw) are benchmarks with trashing memory access patterns that will benefit from more cache (less trashing) and more bandwidth (faster loading of previously trashed data). To be classified as cabw both the requirements of the ca and the bw groups as described above must be satisfied.

\item \textbf{Compute sensitive} (co) are benchmarks limited by the processing power of the simulated processor. These benchmarks do not satisfy the requirements of the ca nor the bw groups as described above.

\end{itemize}

We set the classification limits based on manual observation of benchmark behavior. 
We noted that benchmarks that are bandwidth dependent, in general, had a higher performance loss when we halved the available bandwidth compared to cache sensitive when we halved the cache size.
We also noted that the average performance drop for some benchmarks were dominated by one sample point, in these cases the standard deviation also rose. 
As a result, we applied higher cutoff limits when the standard deviation was high compared to the general case.
Table~\ref{tbl:benchmarks_workloads:benchmark_classification} lists all benchmarks and their classification according to the above rules.


\subsection{Workloads}
\label{sec:methodology:benchmarks:workloads}

Based on the classified benchmarks we generated 4, 8 and 16 core workloads.
The 4 core workloads come in five classes.
One class per benchmark group, these contain workloads only from that particular group.
Also, one class with benchmarks picked from all groups.
The ca and cabw groups contain 5 workloads while the bw and co groups contain 10 workloads.
Table~\ref{tbl:benchmark_workloads:4-workloads} contains an overview of the 50 4 core workloads and their short names used throughout the report.
There is only a single class of 8 and 16 core workloads, as there are not enough benchmarks per group to make workloads of this size. 
The ten 8 and 16 core workloads contain benchmarks from across all benchmark groups.
Table~\ref{tbl:benchmark_workloads:8-workloads} and~\ref{tbl:benchmark_workloads:16-workloads} contains an overview of the 8 and 16 core workloads.

All workloads are generated randomly, but with a few predefined rules.
No benchmark can occur twice within the same workload.
This because we suspect that running two instances of the same benchmark, issuing the same memory operations in lock step, might cause unwanted interference that could skew our results. 
Also, we require that all benchmarks eligible for a workload set must be present in at least one workload in that set.