
\subsection{LRU}
\label{sec:background:algorithms:lru}


\begin{figure}[ht]
    \centering
    \begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
        \hline
        A & B & C & D \\
        \hline
    \end{tabular}
    \begin{tabular}{p{2cm}p{2cm}p{2cm}p{2cm}}
        MRU (3) & 2 & 1 & LRU (0) \\
            &  &  & \bf{Hit C}
    \end{tabular}    

    \begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
        \hline
        C & A & B & D \\
        \hline
    \end{tabular}
    \begin{tabular}{p{2cm}p{2cm}p{2cm}p{2cm}}
        MRU (3) & 2 & 1 & LRU (0) \\
            &  &  & \bf{Miss E}
    \end{tabular}    

    \begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
        \hline
        E & C & A & B \\
        \hline
    \end{tabular}
    \begin{tabular}{p{2cm}p{2cm}p{2cm}p{2cm}}
        MRU (3) & 2 & 1 & LRU (0) \\
            &  &  & \bf{Hit E}
    \end{tabular}    

    \begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
        \hline
        E & C & A & B \\
        \hline
    \end{tabular}
    \begin{tabular}{p{2cm}p{2cm}p{2cm}p{2cm}}
        MRU (3) & 2 & 1 & LRU (0) \\
    \end{tabular}    

    \caption{LRU managed 4-way cache set}
    \label{fig:background:lru_example}
\end{figure}

Least recently used (LRU) replacement, or some simplification of LRU, is one of the dominant cache replacement algorithms in hardware today. 
Normally when presenting a new cache partitioning algorithm, LRU is used as the baseline for comparions~\cite{Jaleel2010,Qureshi2006,Qureshi2007}.
The LRU algorithm relies on the temporal locality of data accesses; it assumes that recently accessed data has a higher reuse frequency then less recently accessed data.
When a cache miss occurs, and a new cache line is stored in a cache set, the LRU algorithm will evict the least recently used cache block.
Theoretically one can envision a cache set managed by LRU as a stack, where recently accessed cache blocks are near the top and less recently accessed blocks are near the bottom.
The bottom position of the stack is the LRU position, and the top position is the most recently used (MRU) position. 
In a hardware implementation, the blocks are not stored in a sorted fashion, but additional storage bits are used to keep track of LRU positions.

Figure~\ref{fig:background:lru_example} shows how a 4-way cache set managed by LRU replacement handles three requests. 
Initially, the set contains four blocks; A, B, C and D. 
A is in the MRU position while D is in the LRU position.
The first request is for block C; this is a hit, and that causes the block to move to the MRU position, pushing both block A and B one step closer to the LRU position.
The second request is made to block E; this block is not present in the cache.
The LRU algorithm evicts block D at the LRU position and then places block E at the MRU position.
Finally, another request is made to block E. Nothing changes since the block already is at the MRU position.

LRU is a simple replacement algorithm that is usable in both private and shared caches.
In shared caches, LRU will favor access frequency, giving cores that issue many cache requests more cache space then those how issue fewer requests. 
In some cases, this might be the an acceptable solution. 
However, several thread aware replacement algorithms claiming to outperform LRU exists. 
In this thesis, we will investigate several such algorithms and compare their performance against the simple LRU algorithm.

\todo{Mention Pseudo-LRU}


\subsection{Common Memmory Access Patterns}
In the following sections, we will present various cache partition algorithms that attempt to improve cache performance compared to the commonly used LRU algorithm.
In order to efficiently explain the strengths of the various partition algorithms, we will first define four different memory access patterns; recency-friendly, trashing, streaming and combined access patterns.

\paragraph{Recency-friendly}
As explained in Section~\ref{sec:background:algorithms:lru} the LRU algorithm assumes that recent memory accesses have a higher chance of reuse than less recent memory accesses.
We call this the recency-assumption, and we call memory access patterns that conform to this assumption recency-friendly access patterns.
Figure~\ref{fig:background:algorithms:rf_pattern} illustrates an example memory access pattern that is said to be recency-friendly.
Assuming LRU replacement, the cache will provide hits for all accesses every time the pattern repeats.

\begin{figure}[ht]
\centering
\begin{equation}
p_{rf} = (a_0 a_1 ... a_{k-1})^N
\end{equation}
\caption{Recency-friendly access pattern to a single cache set (k $<=$ number of ways, N $>$ 1)}
\label{fig:background:algorithms:rf_pattern}
\end{figure}

\paragraph{Trashing}
A trashing memory pattern is one that repeats in a similar manner to a recency-friendly pattern, but with more blocks per cycle than the number of cache ways. 
Figure~\ref{fig:background:algorithms:tr_pattern} shows an example of one such pattern, the only thing separating this from the previous pattern in the value of k.
In an LRU managed cache, an access pattern similar to this will never hit because all blocks are evicted before they are re-referenced.
An optimal replacement algorithm might keep some of the working set in the cache, providing hits for parts of the access pattern.
\begin{figure}[ht]
\centering
\begin{equation}
p_{rf} = (a_0 a_1 ... a_{k-1})^N
\end{equation}
\caption{Trashing memory access pattern to a single cache set (k $>$ number of ways, N $>$ 1)}
\label{fig:background:algorithms:tr_pattern}
\end{figure}

\paragraph{Streaming}
A streaming memory access pattern is one that has not re-references, or where the period is so large that no pattern is detectable.
Figure~\ref{fig:background:algorithms:st_pattern} illustrates one such pattern, where k is infinite.
Neither LRU or any other caching algorithm can provide hits for such an access pattern, simply because there are no re-references.

\begin{figure}[ht]
\centering
\begin{equation}
p_{rf} = (a_0 a_1 ... a_{k-1})
\end{equation}
\caption{Streaming memory access  pattern (k = $\infty$) }
\label{fig:background:algorithms:st_pattern}
\end{figure}

\paragraph{Combined}
In reality, memory access patterns can more complex than the simple examples shown above.
A single program might, for instance, behave both in a recency-friendly and streaming fashion. 
An example would be a program performing a reduction over a large dataset.
Most accesses would be streaming as the program iterates over the dataset, but some accesses will exhibit recency-friendly behavior like when the program stores temporary results to memory during the iteration.

For shared caches the observed pattern is the union of accesses from all cores, this can result in even more complex patterns.
A particular non-optimal situation for an LRU managed shared cache is when multiple cores execute recency-friendly applications, and one single core execute a streaming application.
In this case, the streaming application will constantly clear the cache, degrading performance of the recency-friendly applications.
An algorithm that could detect the one streaming application and handle it as a special case could potentially increase performance of the recency-friendly applications without affecting the performance of the streaming application.
Some of the algorithms we cover in this report will attempt to detect streaming applications.