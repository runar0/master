\subsection{Common Memory Access Patterns}
In the following sections, we will present various cache partition algorithms that attempt to improve cache performance compared to the commonly used LRU algorithm.
In order to efficiently explain the strengths of the various partition algorithms, we will first define four different memory access patterns; recency-friendly, trashing, streaming and combined access patterns.

\paragraph{Recency-friendly}
As explained in Section~\ref{sec:background:algorithms:lru} the LRU algorithm assumes that recent memory accesses have a higher chance of reuse than less recent memory accesses.
We call this the recency-assumption, and we call memory access patterns that conform to this assumption recency-friendly access patterns.
Figure~\ref{fig:background:algorithms:rf_pattern} illustrates an example memory access pattern that is said to be recency-friendly.
Assuming LRU replacement, the cache will provide hits for all accesses every time the pattern repeats.

\begin{figure}[ht]
\centering
\begin{equation}
p_{rf} = (a_0 a_1 ... a_{k-1})^N
\end{equation}
\caption{Recency-friendly access pattern to a single cache set (k $<=$ number of ways, N $>$ 1)}
\label{fig:background:algorithms:rf_pattern}
\end{figure}
\todo{Fix numbering, counts as both an algorithm and figure which is weird.}

\paragraph{Trashing}
A trashing memory pattern is one that repeats in a similar manner to a recency-friendly pattern, but with more blocks per cycle than the number of cache ways. 
Figure~\ref{fig:background:algorithms:tr_pattern} shows an example of one such pattern, the only thing separating this from the previous pattern in the value of k.
In an LRU managed cache, an access pattern similar to this will never hit because all blocks are evicted before they are re-referenced.
An optimal replacement algorithm might keep some of the working set in the cache, providing hits for parts of the access pattern.
\begin{figure}[ht]
\centering
\begin{equation}
p_{rf} = (a_0 a_1 ... a_{k-1})^N
\end{equation}
\caption{Trashing memory access pattern to a single cache set (k $>$ number of ways, N $>$ 1)}
\label{fig:background:algorithms:tr_pattern}
\end{figure}

\paragraph{Streaming}
A streaming memory access pattern is one that has not re-references, or where the period is so large that no pattern is detectable.
Figure~\ref{fig:background:algorithms:st_pattern} illustrates one such pattern, where k is infinite.
Neither LRU or any other caching algorithm can provide hits for such an access pattern, simply because there are no re-references.

\begin{figure}[ht]
\centering
\begin{equation}
p_{rf} = (a_0 a_1 ... a_{k-1})
\end{equation}
\caption{Streaming memory access  pattern (k = $\infty$) }
\label{fig:background:algorithms:st_pattern}
\end{figure}

\paragraph{Combined}
In reality, memory access patterns can more complex than the simple examples shown above.
A single program might, for instance, behave both in a recency-friendly and streaming fashion. 
An example would be a program performing a reduction over a large dataset.
Most accesses would be streaming as the program iterates over the dataset, but some accesses will exhibit recency-friendly behavior like when the program stores temporary results to memory during the iteration.

For shared caches the observed pattern is the union of accesses from all cores, this can result in even more complex patterns.
A particular non-optimal situation for an LRU managed shared cache is when multiple cores execute recency-friendly applications, and one single core execute a streaming application.
In this case, the streaming application will constantly clear the cache, degrading performance of the recency-friendly applications.
An algorithm that could detect the one streaming application and handle it as a special case could potentially increase performance of the recency-friendly applications without affecting the performance of the streaming application.
Some of the algorithms we cover in this report will attempt to detect streaming applications.