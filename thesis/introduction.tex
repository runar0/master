\chapter{Introduction}
\label{cpt:introduction}

In this chapter, we will first introduce chip multiprocessors and their memory systems.
We will also introduce the role of a cache partitioning algorithm, and introduce a set of common memory access patterns used to later discussions.
Then, we will analyze our problem description and introduce a set of requirements the thesis has to fulfill.
Finally, an overview of our contributions is given before we provide an outline of the report.

\section{Chip Multiprocessors}

Moore's Law~\cite{Moore1998}, an observation made by G. E. Moore one of the co-founders of Intel, has been the driving force behind processor development in the last decades.
The law is simply an observation; that scaling of transistors used to make integrated circuits will allow for approximately twice the number of transistors per die every 18 month.
Up until the mid-2000s, manufacturers used these smaller transistors to increase single-core performance.
Smaller transistors allowed for increased frequency, and more transistors per die allowed for increasingly complex processor cores.
Features such as speculative and out-of-order execution were added to take advantage of instruction-level parallelism (ILP) present in computer programs.
By the mid-2000s, processor cores had become so complex and were running at such high frequencies that manufacturers had reached the limitation known as the power wall.
The power wall entails that manufacturers were unable to continue increasing the frequency and the transistor count of each core, without also attaching high-performance cooling systems to counter the increase power usage and hence increase heat generation.
Systems like water or even nitrogen cooling were needed to continue the performance increase~\cite{Sutter2005}; these systems are not practical for personal computers.
Figure~\ref{fig:introduction:memgap} shows the single core performance development for processors from 1980 to 2010.
The effect of the power wall is clearly visible from 2005 and out where we observe no improvement in single core performance.
We also observe that in the five years leading to the power wall, the yearly performance improvement decreased.
This decrease is observed because improving single core performance became harder, as more advanced techniques for ILP exploiting where needed, ultimately resulting in the power wall.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/introduction/memory-gap}
\caption{Comparison of (single core) processor and memory performance from 1980 to 2010 based on data collected by J. Hennessy and D. Patterson\cite{hennessy2012}.}
\label{fig:introduction:memgap}
\end{figure}

By 2005, most manufacturers had abandoned their plans for increased single core performance, and where all working towards chip multiprocessors (CMPs)~\cite{Sutter2005}.
CMPs are built using several simpler processing cores without many of the aggressive ILP utilization features that were added to single core processors in the late 2000s to increase performance.
When removing the aggressive UILP utilization features the per-core power consumption also reduced.
This simplification of cores allowed manufacturers to fit multiple cores on a single die, without reaching the power consumption of a single ILP aggressive processing core.
With an increasing number of transistors due to scaling, even more processing cores can be added to CMPs.
Today CMPs are the de-facto standard, being used in everything from embedded computers~\cite{ARM2010} and mobile phones~\cite{Ho2014} to commercial and high-performance computing~\cite{Thomadakis2011, Jain2013}.


\section{CMP Memory System}

\begin{figure}[ht]
\centering
\includegraphics[scale=.65]{figures/processor_model/processor_model}
\caption{Generic Chip Multiprocessor Architecture.}
\label{fig:cmp_model}
\end{figure}

Memory is a vital component of any computing system, without memory we are unable to store our programs and computations.
Traditionally the technology used to create memories have differed from the technology used in processors~\cite{Wilkes2001}.
As processor performance increased, memory performance did not increase proportionally.
This development has resulted in what is known as the processor-memory gap~\cite{Wilkes2001}.
Figure~\ref{fig:introduction:memgap} shows how the gap in processor and memory performance has developed from 1980 to 2010.
The figure shows that memory performance has had a constant development through the period of about 7% increase yearly.
The processor-memory gap made itself visible in 1986 when processor performance started increasing by about 52% each year.
If not handled the increasing processor-memory gap would become the limiting factor in processor performance, as processors as dependant on the memory system providing both code and data.
In response to the growing gap small and fast memory structures known as caches were introduced.
Caches are produced using different techniques than main memory, proving faster access; this comes at the cost of higher production cost and power usage.
Today, most single-core processors and CMPs have one or more caches integrated on the processor die.
The caches filter all memory request made by the processing core.
If one of the caches has a copy of the requested data, it will stop the memory request from going to main memory and respond with the correct data.
This is known as a cache hit.
If the cache does not have the value of the requested address stored it is known as a cache miss.
Because caches are faster and are located closer to the processor than main memory they can to an extent hide the processor-memory gap, assuming a high hit rate in the caches.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/introduction/dircache_read}
        \caption{Direct mapped cache. Each set contains only one block. The three most significant bits used for set addressing.}
        \label{fig:introduction:cache:dir}
    \end{subfigure}\hfill%
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/introduction/2waycache_read}
        \caption{2-way associative cache. A selector, not shown here, is used to select between line0 and line1 to produce the data output. Each set contain two blocks, and compared to the direct case, additional hardware is required to select the correct output. Two bits used to set addressing, the most significant bit ignored.}
        \label{fig:introduction:cache:2way}
    \end{subfigure}
    \caption{Simplified architecture of a direct and 2-way cache under a read operation. With 64B lines, 7-bit tags, and byte addressing.}
    \label{fig:introduction:cache}
\end{figure}

Traditionally there are three main ways of organizing cache memory; direct, set-associative and fully associative.
Caches used in commercial CMPs today are set associative memory structures~\cite{Thomadakis2011, Jain2013, ARM2010, Ho2014}.
A cache is organized a 2d array, where each row is called a set, and each set consist of one or more blocks, sometimes known as cache lines.
A block is the minimum unit of data a cache stores, this is typically 64B.
The cache divides all memory addresses into three portions, the set address, block tag and block offset.
The set address is used to determine which set is responsible for caching a value.
Within a set, all blocks are valid cache locations.
Each block that contains valid data also stores the block tag of the data it contains.
During a cache lookup, all valid blocks will be scanned looking for one containing the block tag from the address.
In a direct mapped cache, each set contains only one block, making the block scan trivial.
Fully associative caches have all blocks in a single set, and the block scan required that every block be checked.
This is expensive both measured in hardware required, and the critical path of the cache.
Set associative caches, also known as n-way caches, is a middle ground organization that stores n blocks per set.
Figure~\ref{fig:introduction:cache} shows both a direct and a 2-way cache under a read operation.
The figure does not show a fully associative cache organization but given the 2-way cache organization a fully associative cache is created by duplicating the block scan hardware until each column has one row.
In this thesis, we will show that increasing the number of ways can improve performance of a cache and the cache partitioning algorithms.
However, the critical path and complexity of a cache also increases as the number of ways increases.
As a result, first level caches are often limited to 2- or 4-ways~\cite{Sanchez2010}, because of the access latency must be short to prevent the CPU stalling while it waits for data. 
Even third level caches rarely exceed 16- or 32-ways.
Alternative cache organizations have been proposed, such as zcache~\cite{Sanchez2010}, that allows for higher associativity at a lower cost.
However, these techniques are not currently used in commercial CMPs and hence are not considered further in this thesis.

When data not present in the cache is requested by a processing core, a request is sent the cache to the next cache level, or possibly the main memory.
Once the cache receives a response, it normally stores the data to provide faster access times in case of a re-reference.
Caches also normally store data that is written to memory by a processing core, speeding up the write operation.
For a set-associative or fully associative cache, there are multiple valid storage positions for a data block.
An algorithm known as the cache replacement or partitioning algorithm decides which of the valid blocks are used to store the value.
If the block that was chosen by the algorithm already contains valid data the cache removes or evicts the existing data.
While cache replacement and cache partitioning algorithms may be used to refer to slightly different types of algorithms, we will use the term cache partitioning algorithm throughout this thesis to reduce confusion.
The introduction to chapter~\ref{cpt:algorithms} covers the difference between a replacement and partitioning algorithm.

In the memory hierarchy, smaller means faster. 
For instance, main memory is much faster than disks, but disks can store much more data.
The same is valid for caches, a smaller cache has a shorter critical path and hence a lower access time compared to a larger cache.
Figure~\ref{fig:cmp_model} shows an example 16-core CMP architecture with three cache levels and main memory.
For each cache level, the size increased and access time increases.
The first level caches are the smallest while the third level caches are largest.
In order to save area on CMP chips, and also to provide an easy mechanism for data sharing between cores, it is common to have at least one level of shared cache.
In figure~\ref{fig:cmp_model} each core has a private L1 and L2 cache, and a shared last-level cache (LLC), the L3 cache.
While sharing cache can improve performance, and improve overall utilization, it also makes the memory system exposed to destructive interference that potentially can hurt the performance of all processing cores. 
The cache partitioning algorithm, running on the shared cache, may either ignore the effects of interference or it may attempt to reduce it by some form of prioritization.

\section{Common Memory Access Patterns}

In this section, we will define four memory access patterns~\cite{Jaleel2010}; recency-friendly, trashing, streaming and combined access patterns.
These patterns will later be used to describe the strengths and weaknesses of presented algorithms, and to explain algorithm performance in our experiments.

\subsection{Recency-friendly}
Several cache partitioning algorithms make an assumption known as the recency-assumption.
This is the assumption that recently accesses memory addresses have a higher probability of re-use that less recently accesses addresses.
A recency-friendly access pattern is one that causes no misses under an algorithm that always keeps the most recently used blocks in the cache.
Figure~\ref{fig:algorithms:rf_pattern} illustrates an example memory access pattern that is said to be recency-friendly.
Because the pattern repeats and $k$ is less then the number of ways we know that the least recently used block will always be referenced before being evicted, if we assume a recency-assumption based algorithm.
Hence, the algorithm will be able to provide hits for every access after the initial repetition and the pattern is recency-friendly.

\begin{figure}[ht]
\centering
\begin{equation} \label{fig:algorithms:rf_pattern}
p_{rf} = (a_0 a_1 ... a_{k-1})^N
\end{equation}
\caption{Recency-friendly access pattern (k $<=$ number of ways, N $>$ 1).}
\end{figure}

\subsection{Trashing}
A trashing memory pattern is one that repeats in a similar manner to a recency-friendly pattern, but with more blocks per cycle than the number of cache ways. 
Figure~\ref{fig:algorithms:tr_pattern} shows an example of one such pattern, the only thing separating this from the previous pattern in the value of $k$.
Under LRU or another recency-assumption based algorithm, an access pattern similar to this will never hit because all blocks are evicted before they are re-referenced.
A better replacement algorithm for these patterns might keep some of the working set in the cache, providing hits for parts of the access pattern.
\begin{figure}[ht]
\centering
\begin{equation} \label{fig:algorithms:tr_pattern}
p_{rf} = (a_0 a_1 ... a_{k-1})^N
\end{equation}
\caption{Trashing memory access pattern (k $>$ number of ways, N $>$ 1).}
\end{figure}

\subsection{Streaming}
A streaming memory access pattern is one that has no re-references, or where the period is so large that no pattern is detectable.
Figure~\ref{fig:algorithms:st_pattern} illustrates one such pattern, where k is infinite.
No caching algorithm can provide hits for such an access pattern, simply because there are no re-references.

\begin{figure}[ht]
\centering
\begin{equation} \label{fig:algorithms:st_pattern}
p_{rf} = (a_0 a_1 ... a_{k-1})
\end{equation}
\caption{Streaming memory access  pattern (k = $\infty$).}
\end{figure}

\subsection{Combined}
In reality, memory access patterns can be more complex than the simple examples shown above.
A single program might, for instance, behave both in a recency-friendly and streaming fashion. 
An example would be a program performing a reduction over a large dataset.
Most accesses would be streaming as the program iterates over the dataset, but some accesses will exhibit recency-friendly behavior like when the program stores temporary results to memory during the iteration.

For shared caches the observed pattern is the union of accesses from all cores, this can result in even more complex patterns.
A particular non-optimal situation for a cache assuming recency-friendly behavior is when multiple cores execute recency-friendly applications, and one single core execute a streaming application.
In this case, the streaming application will constantly clear the cache, degrading performance of the recency-friendly applications.
An algorithm that could detect the one streaming application and handle it as a special case could potentially increase the performance of the recency-friendly applications without affecting the performance of the streaming application.
Some of the algorithms we cover in the following sections will attempt to detect streaming applications.

\section{Requirements}
By analyzing the problem description, we have been able to extract a set of requirements that this thesis has to fulfill:

\begin{description}
    \item[R1] Introduce CMPs, their memory system, and the role of a cache partitioning algorithm.
    \item[R2] Present recent and important work in the cache partitioning field. Compare similarities and differences of the various proposed algorithms.
    \item[R3] Create a framework for evaluation of various cache partitioning algorithms.
    \item[R4] Implement at least one of the presented algorithm and compare against a conventional LRU-managed cache.
\end{description}

Also, based on the problem description and discussions with the thesis advisors we list an additional set of optional requirements that the thesis may fulfill:

\begin{description}
    \item[O1] Implement additional algorithms and evaluate them.
    \item[O2] Compare performance of the implemented algorithms against each other.
    \item[O3] Investigate algorithm sensitivity to changes in L2 cache, L3 cache, and/or memory bus bandwidth.
\end{description}

\section{Requirements}

In our work with this thesis we have made the following contributions:

\begin{itemize}
  \item We have created a curated list with detailed descriptions of several recently published cache partitioning algorithms. The list has been limited to only consider algorithms that target conventional caches and optimize of cache miss reduction.
  \item A framework for evaluation of cache partitioning algorithms have been built on top of the Sniper~\cite{Carlson2011a} simulation system.
  \item Several of the algorithms presented in our list has been implemented and tested within our framework.
  \item We performed several sensitivity experiments further exploring the properties of our simulation framework and the implemented algorithms.
  \item The implemented simulation framework will at the end of this thesis be made available to NTNU and the CARD~\cite{CARD2015} research group for future research.
\end{itemize}

\section{Outline}

The outline of the rest of this thesis is as follows:

\begin{itemize}
  \item Chapter~\ref{cpt:introduction} introduces the thesis by putting it in a historical context, presenting some background knowledge and presenting the contributions made, fulfilling requirement \textbf{R1}.

  \item Chapter~\ref{cpt:algorithms} presents a selection of cache partitioning algorithms and provides a theoretical comparison of them, fulfilling requirement \textbf{R2}.

  \item Chapter~\ref{cpt:framework} presents our simulator and the framework built on this simulator to enable evaluation of cache partitioning algorithms. It also presents the subset of algorithms we have implemented. This fulfills requirement \textbf{R3} and \textbf{O1}.

  \item Chapter~\ref{cpt:methodology} contains a description of our simulated processor model and explains all metrics we later use to evaluate our experiments.

  \item Chapter~\ref{cpt:results} presents an experiment where we compare all implemented algorithms against an LRU managed cache. Also, we compare all implemented algorithms against each other. This fulfills requirement \textbf{R4} and \textbf{O2}.

  \item Chapter~\ref{cpt:sresults} presents five different experiments all exploring either framework or algorithm sensitivity to various architectural changes, fulfilling requirement \textbf{O3}.

  \item Chapters~\ref{cpt:discussion} and~\ref{cpt:conclusion} contains a discussion of our results and a conclusion based on these. Also, an overview of future work is given.

\end{itemize}