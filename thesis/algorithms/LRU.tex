
\section{LRU}
\label{sec:algorithms:lru}


\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/algorithms/LRU}
    \caption{LRU managed 4-way cache set}
    \label{fig:background:lru_example}
\end{figure}

Least recently used (LRU) replacement, or some simplification of LRU, is one of the dominant cache management algorithms in hardware today. 
As a result, LRU is normally used as the baseline for comparisons when presenting new cache management algorithms~\cite{Jaleel2010,Qureshi2006,Qureshi2007}.

The LRU algorithm relies on the temporal locality of data accesses; it assumes that recently accessed data has a higher reuse frequency then less recently accessed data.
Theoretically one can envision a cache set managed by LRU as a stack, where recently accessed cache blocks are near the top and less recently accessed blocks are near the bottom.
The bottom position of the stack is the LRU position, and the top position is the most recently used (MRU) position.
In a hardware implementation, the blocks are not stored in a sorted fashion, but additional storage bits are used to keep track of LRU positions.
The replacement policy of LRU is to evict the least recently used cache block, the one on the bottom of the LRU stack.
The insertion and promotion policy of LRU is the same; a inserted or accessed block is always moved to the MRU position unless it is already there.

Figure~\ref{fig:background:lru_example} shows how a 4-way cache set managed by LRU replacement handles three requests. 
Initially, the set contains four blocks; A, B, C and D. 
A is in the MRU position while D is in the LRU position.
The first request is for block C; this is a hit, and that causes the block to move to the MRU position, pushing both block A and B one step closer to the LRU position.
The second request is made to block E; this block is not present in the cache.
The LRU algorithm evicts block D at the LRU position and then places block E at the MRU position.
Finally, another request is made to block E. Nothing changes since the block already is at the MRU position.

LRU is a simple replacement algorithm that is usable in both private and shared caches.
In shared caches, LRU will favor access frequency, giving cores that issue many cache requests more cache space than those who issue fewer requests. 
In some cases, this might be an acceptable solution. 
However, as we will discover, several thread-aware replacement algorithms claiming to outperform LRU exists. 


