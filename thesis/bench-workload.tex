
\chapter{Bechmarks and Workloads}
\label{cpt:benchmarks_workloads}

In this chapter, we will present the benchmarks we use to evaluate cache partitioning algorithms.
We explain how the we extracted simulation traces from benchmarks and how we created 4-, 8-, and 16-core workloads based on those traces.

\section{Benchmarks}

In all our experiments, we are utilizing benchmarks from the SPEC CPU2006\todo{cite} benchmark suite. 
We choose this suite because it is the newest of the CPU benchmark suites from SPEC, and it is specifically designed to test performance of various computer architectures using workloads based on real user applications.
Unless otherwise stated all benchmarks use the first reference input set.
For each benchmark, we use SimPoint\todo{cite} (Section~\ref{sec:background:simpoint}) to extract intervals that we use to represent the entire benchmark.
In this work, we choose to extract only a single interval from each benchmark.
By doing this, we are willingly increasing the error\todo{cite} between simulating our interval and the results obtained by simulating the entire benchmark.
Because we are investigating performance changes due to various architectural choices, we find that this increased error will not affect our results in a negative manner.
Having only one interval per benchmark greatly simplifies the process of simulation.
We only have to do one simulation per benchmark, and the results of this simulation are equal to the weighted results as the weight will always equal one.

The length chosen for our intervals may also affect the final results.
First, as caches are empty when simulation starts the cold cache effect\todo{cite from paper}, caused by compulsory misses at simulation start, may skew our results.
Additionally, as we are experimenting with cache partitioning algorithms, we believe that a certain number of instructions are needed for our results to stabilize.
Finally, by increasing the number of instructions we are also increasing the time required to simulate a benchmark.
Based on these observations we choose to run SimPoint with an interval size of 250M instructions.
This number of instructions should make the cold cache effect negishable\todo{paper cite} while we keep the simulation time relatively low.
For each SimPoint extracted interval, a trace file, containing the instruction stream within the interval, is generated using Sniper.
We use this trace files for all later experiments.

\section{Benchmark Classification}

In order to better understand our simulation results, we perform a benchmark classification experiment on each of the previously generated traces.
This experiment is intended to detect various properties in each trace that may affect how they behave on our simulated architecture with various cache partitioning algorithms.
We choose to categorize traces based on their sensitivity to the size of the LLC and the speed of the bus connecting the LLC and the DRAM.

The system model used in this experiment is as shown in Table~\ref{tbl:processor_model:properties} with the smallest of the L2 configurations, 128KB.
The size of the L3 cache and the speed of the memory bus is varied as shown in Table~\ref{tbl:benchmarks_workloads:classification_model}.
By reducing the size of the L3 cache, and the speed of the memory bus we intend to simulate a situation where the benchmark has access to reduced resources due to contention. 

\begin{table}[ht]
\centering
\begin{tabular}{rl}
\toprule
\bf{L3 Cache size} & 256kB, 512kB, 1024kB, 2048kB, 4096kB \\
\bf{Memory Bus Speed}   & 1.6GB/s, 3.2GB/s, 6.4GB/s, 12.8GB/s \\
\bottomrule                             
\end{tabular}
\caption{Model properties}
\label{tbl:processor_model:properties}
\end{table}

Each benchmark is simulated for each combination of L3 size and memory bus speed, in total 20 simulations per benchmark.
We then evaluate how changes to the architecture affected the benchmarks performance using the reported IPC.
This is done by calculating an average IPC difference between each configuration pair.
For the cache, we find the IPC drop between the 4MB and 2MB configuration, the 2MB and 1MB configuration, etc.
We repeat the process for each memory configuration.
This results in an average IPC drop due to a cache size reduction, as well as a standard deviation.
Numbers of reductions in memory bandwidth are found using the same methodology.

Based on the average IPC drop and the standard deviation we classify each trace into one of four categories:
\begin{enumerate}

\item \textbf{Cache sensitive} (ca) benchmarks have an average performance drop of at least 4\% if the standard deviation is less than 11\% or a performance drop of at least 13\% otherwise. These are in general benchmarks with memory access patterns that are recency-friendly.

\item \textbf{Bandwidth sensitive} (bw) benchmarks have an average performance drop of at least 8\% if the standard deviation is less than 11\% or a performance drop of at least 20\% otherwise. These are in general benchmarks that have memory access patterns with no or little temporal locality, often streaming patterns.

\item \textbf{Cache- and Bandwidth sensitive} (cabw) benchmarks are benchmarks that fit in both of the above classes. These are often benchmarks with trashing memory access patterns, that will benefit from more cache (i.e. less trashing) and more bandwidth (faster loading of previously trashed data)

\item \textbf{Compute sensitive} (co) benchmarks are those that fit into none of the above categories. These are benchmarks limited by the computational power of the processor model and not the shared memory system.

\end{enumerate}

We set the classification limits based on manual observation of benchmark behavior. 
We noted that benchmarks that are bandwidth dependant in general had a higher performance loss when we halved the available bandwidth compared to cache sensitive when we halved the cache size.
We also noted that the average performance drop for some benchmarks were dominated by one sample point, in these cases the standard deviation also rose. 
As a result, we applied higher cutoff limits when the standard deviation was high compared to the general case.
Table~\ref{tbl:benchmarks_workloads:benchmark_classification} lists all benchmarks and their classification according to the above rules.

\input{figures/workloads/benchmarks.tex}

\section{Workloads}

\input{figures/workloads/workloads.tex}

\todo{Generate new workloads, this time 4, 8, 16 core workloads}
\todo{How do we handle 8 and 16 if there are less than that in a group?}