
\chapter{Bechmarks and Workloads}
\label{cpt:benchmarks_workloads}

In this chapter, we will present the benchmarks we use to evaluate cache partitioning algorithms.
We explain how the we extracted simulation traces from benchmarks and how we classified those traces based on their sensitivity to changes in available cache space and memory bandwidth. 
Finally, we explain how we created 4-, 8-, and 16-core workloads based on those benchmark traces.

\section{Benchmarks}

In all our experiments, we are utilizing benchmarks from the SPEC CPU2006~\cite{SPECCPU2006} benchmark suite. 
We choose this suite because it is the newest of the CPU benchmark suites from SPEC, and it is specifically designed to test performance of various computer architectures with benchmarks based on real user applications.
Unless otherwise stated all benchmarks use the first reference input set.
For each benchmark, we use SimPoint\cite{Hamerly2005} to extract intervals that we use to represent the entire benchmark.
In this work, we choose to extract only a single interval from each benchmark.
By doing this, we are willingly increasing the error\cite{Hamerly2004} between simulating our interval and the results obtained by simulating the entire benchmark.
Because we are investigating performance changes due to various architectural choices, we find that this increased error will not affect our results in a negative manner.
Having only one interval per benchmark greatly simplifies the process of simulation.
We only have to do one simulation per benchmark, and the results of this simulation are equal to the weighted results as the weight will always equal one.

The length chosen for our intervals may also affect the final results.
As caches are empty when simulation starts the cold cache effect, caused by compulsory misses at simulation start, may skew our results if the simulated interval is short.
Additionally, as we are experimenting with cache partitioning algorithms, we believe that a certain number of instructions are needed for our results to stabilize.
Finally, by increasing the number of instructions we are also increasing the time required to simulate a benchmark.
We choose to extract 250M instruction intervals using SimPoints.
This number of instructions will make the cold cache effect neglishable~\cite{Hamerly2005,Hamerly2004,Olsen2014} while we keep the simulation time relatively low.
We generate an instruction trace using Sniper for each SimPoint extracted interval.
All later experiments utilize these traces in place of the actual benchmark executable.

\section{Benchmark Classification}

\input{figures/workloads/benchmarks.tex}

In order to better understand our simulation results, we perform a benchmark classification experiment on each of the previously generated traces.
This experiment is intended to detect various properties in each trace that may affect how they behave on our simulated architecture with various cache partitioning algorithms.
We choose to categorize traces based on their sensitivity to the size of the LLC and the bandwidth of the bus connecting the LLC and the DRAM.

The system model used in this experiment is as shown in Table~\ref{tbl:processor_model:properties} with the smallest of the L2 configurations, 128KB.
The size of the L3 cache and the speed of the memory bus is varied as shown in Table~\ref{tbl:benchmarks_workloads:classification_model}.
By reducing the size of the L3 cache and the speed of the memory bus, we intend to simulate a situation where the benchmark has reduced access to resources due to contention.

\begin{table}[ht]
\centering
\begin{tabular}{rl}
\toprule
\bf{L3 Cache size} & 256kB, 512kB, 1024kB, 2048kB, 4096kB \\
\bf{Memory Bus Speed}   & 1.6GB/s, 3.2GB/s, 6.4GB/s, 12.8GB/s \\
\bottomrule                             
\end{tabular}
\caption{Model properties}
\label{tbl:benchmarks_workloads:classification_model}
\end{table}

We simulate each benchmark for each combination of LLC size and memory bus bandwidth, in total 20 simulations per benchmark.
Then we evaluate how changes to the architecture affected the benchmarks performance using the reported IPC.
The evaluation is done by organizing the IPC measurements in a 2d data table with LLC size on one axis and bandwidth on the other.
Between each data pair along each axis, we calculate the performance reduction.
Using the arithmetic average of reductions along each axis we classify each benchmark as either sensitive or not sensitive to changes in that axis property.
The standard deviation is used to increase the required limits on the average reduction if a benchmark shows high variability along an axis.
This often happens in cases where there is little change in performance except for one point where performance is significantly worse, dragging the average value down.
We define four groups for our benchmarks as describe below.
For each group, we define rules used to select benchmarks into that group.
\begin{itemize}

\item \textbf{Cache sensitive} (ca) benchmarks are in general benchmarks with memory access patterns that are recency-friendly. We required an average performance reduction along the cache axis of at least 4\% and a standard deviation of less than 11\% to classify a benchmark as cache sensitive. If the deviation is higher than 11\%, a performance reduction of at least 13\% is required to be classified as cache sensitive.

\item \textbf{Bandwidth sensitive} (bw) are benchmarks with no to little temporal locality, often streaming access patterns. We required an average performance reduction along the bandwidth axis of at least 8\% and a standard deviation of less than 11\% to classify a benchmark as bandwidth sensitive. If the deviation is higher than 11\%, an average performance reduction of at least 20\% is required to be classified as bandwidth sensitive. 

\item \textbf{Cache- and Bandwidth sensitive} (cabw) are benchmarks with trashing memory access patterns that will benefit from more cache (less trashing) and more bandwidth (faster loading of previously trashed data). To be classified as cabw both the requirements of the ca and the bw groups as described above must be satisfied.

\item \textbf{Compute sensitive} (co) are benchmarks limited by the processing power of the simulated processor. These benchmarks do not satisfy the requirements of the ca nor the bw groups as described above.

\end{itemize}

We set the classification limits based on manual observation of benchmark behavior. 
We noted that benchmarks that are bandwidth dependant in general had a higher performance loss when we halved the available bandwidth compared to cache sensitive when we halved the cache size.
We also noted that the average performance drop for some benchmarks were dominated by one sample point, in these cases the standard deviation also rose. 
As a result, we applied higher cutoff limits when the standard deviation was high compared to the general case.
Table~\ref{tbl:benchmarks_workloads:benchmark_classification} lists all benchmarks and their classification according to the above rules.


\section{Workloads}

Based on the classified benchmarks we generated 4, 8 and 16 core workloads.
The 4 core workloads come in five classes.
One class per benchmark group, these contain workloads only from that particular group.
Also, one class with benchmarks picked from all groups.
The ca and cabw groups contain 5 workloads while the bw and co groups contain 10 workloads.
Table~\ref{tbl:benchmark_workloads:4-workloads} contains an overview of the 50 4 core workloads and their short names used throughout the report.
There is only a single class of 8 and 16 core workloads, as there are not enough benchmarks per group to make workloads of this size. 
The ten 8 and 16 core workloads contain benchmarks from across all benchmark groups.
Table~\ref{tbl:benchmark_workloads:8-workloads} and~\ref{tbl:benchmark_workloads:16-workloads} contains an overview of the 8 and 16 core workloads.

All workloads are generated randomly, but with a few predefined rules.
No benchmark can occur twice within the same workload.
This because we suspect that running two instances of the same benchmark, issuing the same memory operations in lock step, might cause unwanted interference that could skew our results. 
In addition, we require that all benchmarks eligible for a workload set must be present in at least one workload in that set.