
\chapter{Bechmarks and Workloads}
\label{cpt:benchmarks_workloads}

In this chapter, we will present the benchmarks we use to evaluate cache partitioning algorithms.
We explain how the we extracted simulation traces from benchmarks and how we classified those traces based on their memory and cache dependencies. Finally, we explain how we created 4-, 8-, and 16-core workloads based on those benchmark traces.

\section{Benchmarks}

In all our experiments, we are utilizing benchmarks from the SPEC CPU2006~\cite{SPECCPU2006} benchmark suite. 
We choose this suite because it is the newest of the CPU benchmark suites from SPEC, and it is specifically designed to test performance of various computer architectures using workloads based on real user applications.
Unless otherwise stated all benchmarks use the first reference input set.
For each benchmark, we use SimPoint\cite{Hamerly2005} (Section~\ref{sec:background:simpoint}) to extract intervals that we use to represent the entire benchmark.
In this work, we choose to extract only a single interval from each benchmark.
By doing this, we are willingly increasing the error\cite{Hamerly2004} between simulating our interval and the results obtained by simulating the entire benchmark.
Because we are investigating performance changes due to various architectural choices, we find that this increased error will not affect our results in a negative manner.
Having only one interval per benchmark greatly simplifies the process of simulation.
We only have to do one simulation per benchmark, and the results of this simulation are equal to the weighted results as the weight will always equal one.

The length chosen for our intervals may also affect the final results.
First, as caches are empty when simulation starts the cold cache effect, caused by compulsory misses at simulation start, may skew our results.
Additionally, as we are experimenting with cache partitioning algorithms, we believe that a certain number of instructions are needed for our results to stabilize.
Finally, by increasing the number of instructions we are also increasing the time required to simulate a benchmark.
Based on these observations we choose to run SimPoint with an interval size of 250M instructions.
This number of instructions will make the cold cache effect neglishable~\cite{Hamerly2005,Hamerly2004,Olsen2014} while we keep the simulation time relatively low.
We generate an instruction trace using Sniper for each SimPoint extracted interval.
All later experiments utilize these traces in place of the actual benchmark executable.

\section{Benchmark Classification}

\input{figures/workloads/benchmarks.tex}

In order to better understand our simulation results, we perform a benchmark classification experiment on each of the previously generated traces.
This experiment is intended to detect various properties in each trace that may affect how they behave on our simulated architecture with various cache partitioning algorithms.
We choose to categorize traces based on their sensitivity to the size of the LLC and the speed of the bus connecting the LLC and the DRAM.

The system model used in this experiment is as shown in Table~\ref{tbl:processor_model:properties} with the smallest of the L2 configurations, 128KB.
The size of the L3 cache and the speed of the memory bus is varied as shown in Table~\ref{tbl:benchmarks_workloads:classification_model}.
By reducing the size of the L3 cache and the speed of the memory bus, we intend to simulate a situation where the benchmark has reduced access to resources due to contention.

\begin{table}[ht]
\centering
\begin{tabular}{rl}
\toprule
\bf{L3 Cache size} & 256kB, 512kB, 1024kB, 2048kB, 4096kB \\
\bf{Memory Bus Speed}   & 1.6GB/s, 3.2GB/s, 6.4GB/s, 12.8GB/s \\
\bottomrule                             
\end{tabular}
\caption{Model properties}
\label{tbl:benchmarks_workloads:classification_model}
\end{table}

We simulate each benchmark for each combination of L3 size and memory bus speed, in total 20 simulations per benchmark.
We then evaluate how changes to the architecture affected the benchmarks performance using the reported IPC.
This is done by calculating an average IPC difference between each configuration pair.
For the cache, we find the IPC drop between the 4MB and 2MB configuration, the 2MB and 1MB configuration, etc.
We repeat the process for each memory configuration.
This results in an average IPC drop due to a cache size reduction, as well as a standard deviation.
Numbers of reductions in memory bandwidth are found using the same methodology.

Based on the average IPC drop and the standard deviation we classify each trace into one of the four categories:
\begin{itemize}

\item \textbf{Cache sensitive} (ca) benchmarks have an average performance drop of at least 4\% if the standard deviation is less than 11\% or a performance drop of at least 13\% otherwise. These are in general benchmarks with memory access patterns that are recency-friendly.

\item \textbf{Bandwidth sensitive} (bw) benchmarks have an average performance drop of at least 8\% if the standard deviation is less than 11\% or a performance drop of at least 20\% otherwise. These are in general benchmarks that have memory access patterns with no or little temporal locality, often streaming patterns.

\item \textbf{Cache- and Bandwidth sensitive} (cabw) benchmarks are benchmarks that fit in both of the above classes. These are often benchmarks with trashing memory access patterns, that will benefit from more cache (i.e. less trashing) and more bandwidth (faster loading of previously trashed data)

\item \textbf{Compute sensitive} (co) benchmarks are those that fit into none of the above categories. These are benchmarks limited by the computational power of the processor model and not the shared memory system.

\end{itemize}

We set the classification limits based on manual observation of benchmark behavior. 
We noted that benchmarks that are bandwidth dependant in general had a higher performance loss when we halved the available bandwidth compared to cache sensitive when we halved the cache size.
We also noted that the average performance drop for some benchmarks were dominated by one sample point, in these cases the standard deviation also rose. 
As a result, we applied higher cutoff limits when the standard deviation was high compared to the general case.
Table~\ref{tbl:benchmarks_workloads:benchmark_classification} lists all benchmarks and their classification according to the above rules.


\section{Workloads}

\input{figures/workloads/workloads-4.tex}
\input{figures/workloads/workloads-8.tex}
\input{figures/workloads/workloads-16.tex}

Based on the classified benchmarks we generated 4, 8 and 16 core workloads.
The 4 core workloads come in five classes.
One class per benchmark group, these contain workloads only from that particular group.
Also, one class with benchmarks picked from all groups.
Each workload class contains 10 workloads.
Table~\ref{tbl:benchmark_workloads:4-workloads} contains an overview of all 4 core workloads and their short names used throughout the report.
There is only a single class of 8 and 16 core workloads, as there are not enough benchmarks per group to make workloads of this size. 
The ten 8 and 16 core workloads contains benchmarks from across all benchmark groups.
Table~\ref{tbl:benchmark_workloads:8-workloads} and~\ref{tbl:benchmark_workloads:16-workloads} contains an overview of the 8 and 16 core workloads.

All workloads are generated randomly, but with a few predefined rules.
No benchmark can occur twice within the same workload.
This because we suspect that running two instances of the same benchmark, issuing the same memory operations in lock step, might cause unwanted interference that could skew our results. 
In addition, we require that all benchmarks eligible for a workload set be present in at least one workload in that set.