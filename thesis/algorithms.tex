
\chapter{Cache Partitioning Algorithms}
\label{cpt:algorithms}

\begin{table}[h]
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
\multicolumn{1}{|c|}{Algorithm} & \multicolumn{1}{c|}{Published} & \multicolumn{1}{c|}{Thread} & \multicolumn{1}{c|}{Replacement} & \multicolumn{1}{c|}{Insertion} & \multicolumn{1}{c|}{Promotion} & \multicolumn{1}{c|}{Hardware}         \\
\multicolumn{1}{|c|}{}          & \multicolumn{1}{c|}{}          & \multicolumn{1}{c|}{aware}  & \multicolumn{1}{c|}{policy}      & \multicolumn{1}{c|}{policy}    & \multicolumn{1}{c|}{policy}    & \multicolumn{1}{c|}{overhead\footnote{Simplified hardware overhead compared to a LRU managed cached.}}         \\ \hline
DIP                             & 2007                           & No                          & LRU                              & LIP/BIP                        & Promote to MRU/Skip            & 1 counter. set dueling                \\ \hline
TADIP                           & 2008                           & Yes                         & LRU                              & LIP/BIP                        & Promote to MRU/Skip            & 1 counter/core, set dueling           \\ \hline
DRRIP                           & 2010                           & Yes                         & LRU approx.                      & SRRIP/BRRIP                    & Frequency promotion            & 1 counter/core, set dueling           \\ \hline
UCP                             & 2006                           & Yes                         & Per core LRU                     & LRU                            & Promote to MRU                 & UMON 1 ATD per core                   \\ \hline
PIPP                            & 2009                           & Yes                         & LRU                              & Utility position               & Frequency promotion            & UMON 1 ATD per core, random generator \\ \hline
PriSM                           & 2012                           & Yes                         & Random core LRU                  & LRU                            & Promote to MRU                 & 1 ATD per core                        \\ \hline
NUCache                         & 2011                           & No                          & LRU, with DeliWays promotion     & LRU                            & Promote to MRU                 & NUTrack                               \\ \hline
CLU                             & 2014                           & Yes                         & LRU                              & LIP/BIP                        & Promote to MRU/Skip            & UMON \~3 ATDs per core                 \\ \hline
\end{tabular}
\label{tbl:algorithms}
\caption{Overview of Cache Partitioning Algorithms}
\end{table}

A cache partitioning algorithm divides available cache space between core(s) that access that cache.
Algorithms often enforce the division via policies that define how to insert new data and how and when to replace old data.
Some algorithms are thread-aware and geared towards shared caches.
Others are thread-agnostic and work both for shared and private caches.
Some have advanced optimization goals such as quality of service (QoS) while others use simpler metrics like miss minimization.
In the following sections, we will present various algorithms primarily proposed to manage shared caches.
We will also present LRU, a caching algorithm that is thread-agnostic and widely used both in private and shared caches today.

There exists a large number of works regarding cache partitioning. 
In this thesis, we select a few algorithms based on some of their characteristics and compare their performance.
Firstly we select only algorithms that optimize for fewer cache misses.
This metric is easy to measure and also makes it easy to compare the various algorithms.
We also focus on algorithms that target conventional caches as they are designed in CMPs today.
This limitation makes the comparison of results from different algorithms easier.
Also, we avoid having to extend our simulator with a new cache type, which would unfeasible given the time constraints of this thesis.
Table~\ref{tbl:algorithms} is an overview of all selected algorithms presented in this thesis.

It is possible to divide all algorithms includes in this evaluation into three distinct policies.
\begin{itemize}
\item The replacement policy specifies which block a cache set evicts when inserting a new block into that set.
\item The insertion policy specifies the state of new blocks after insertion into the cache set.
\item The promotion policy specifies how the state of a block changes following an access from a processor core.
\end{itemize}
In the following sections, we will explain how each of the selected cache partitioning algorithms work, with an emphasis on this division to make comparisons easier.

\input{algorithms/common_patterns.tex}
\input{algorithms/LRU.tex}
\input{algorithms/DIP.tex}
\input{algorithms/TADIP.tex}
\input{algorithms/DRRIP.tex}
\input{algorithms/UCP.tex}
\input{algorithms/PIPP.tex}
\input{algorithms/PriSM.tex}
\input{algorithms/NUCache.tex}
\input{algorithms/CLU.tex}