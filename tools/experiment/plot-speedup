#!/usr/bin/env python

import argparse
import os
import numpy as np
from matplotlib import pyplot as plt
from matplotlib import cm
import matplotlib
#matplotlib.rcParams.update({'font.size': 20})

from os import path
from sys import exit
import collections

from build import profiles
from benchmarks import benchmark_sets

def extract_ipc(filename, coreno = 0):
	instructions = extract_property('performance_model.instruction_count', filename, coreno)
	cycles = extract_property('performance_model.cycle_count', filename, coreno)
	if cycles == None: return 0.1
	return instructions/cycles

def extract_cycles(filename, coreno = 0):
	return extract_property('performance_model.cycle_count', filename, coreno)

def extract_property(property, filename, coreno = 0):
	try:
		with open(filename) as f:
			for line in f:
				line = line.split(' = ')
				if line[0] == property:
					return float(line[1].split(',')[coreno])
	except:
		return 0
	return 0

#print profiles

parser = argparse.ArgumentParser(description='Workload speedup/throughput grapher')
parser.add_argument('--dir', default='.', help='Experiment run directory')
parser.add_argument('--out', default='_figures/speedup/', help='Figure output directory')

args = parser.parse_args()

if not path.exists(args.dir):
	exit('Run directory %s does not exist!' % args.dir)

if not path.exists(args.out):
	os.makedirs(args.out)
	print 'Created output directory %s' % args.out

rundir = path.realpath(args.dir)

benchmark_results = { 'lru': {} }
workload_results = { 'lru': {}, 'tadip': {}, 'drrip': {}, 'ucp': {}, 'pipp': {}, 'pipp-nostream': {} }

for subdir, dirs, files in os.walk(rundir):
	for name in dirs:
		#if "l2-0128k" not in name: continue

		run = name.split('.')
		run_name = run[0]

		# Filter out invalid folders
		if run_name.startswith('_') or len(run) == 1:
			continue;

		benchmarks = run_name.split('-')
		algorithm = run[2]

		if len(benchmarks) == 1:
			benchmark_results[algorithm][run_name] = extract_cycles('%s/%s/stats-benchmark-0.txt' % (rundir, name))
		else:
			results = {}
			i = 0
			for benchmark in benchmarks:
				results[benchmark] = extract_cycles('%s/%s/stats-benchmark-%d.txt' % (rundir, name, i), i)
				i = i + 1
			workload_results[algorithm][run_name] = results

# Transform workload results to STP and ANTT measurements
speedup_results = {}
for algorithm, runs in workload_results.items():
	speedup_results[algorithm] = {}

	for run_name, results in runs.items():
		stp = 0
		antt = 0

		for benchmark, cycles in results.items():
			antt = antt + cycles/benchmark_results['lru'][benchmark]
			stp = stp + benchmark_results['lru'][benchmark]/cycles if cycles > 0 else 0

		antt = antt / len(results)

		speedup_results[algorithm][run_name] = {
			'stp': stp,
			'antt': antt
		}

#print speedup_results

# Normalize to LRU results
#normalized_results = {}
#for algorithm, runs in speedup_results.items():
#	normalized_results[algorithm] = {}
#
#	for run_name, results in runs.items():
#		res = {}
#		if algorithm == 'lru':
#			res = {
#				'stp': 1,
#				'antt': 1
#			}
#		else:
#			res = {
#				'stp': results['stp']/speedup_results['lru'][run_name]['stp'],
#				'antt': results['antt']/speedup_results['lru'][run_name]['antt'],
#			}
#		normalized_results[algorithm][run_name] = res

# convert run names to group names
aliases = [	
	('workload-cache', 'ca4', 'Cache'),
	#('workload-cache-bw', 'cabw4', 'Cache-Bandwidth'),
	#('workload-bw', 'bw4', 'Bandwidth'),
	#('workload-compute', 'co4', 'Compute'),
	#('workload-random', 'ra4', 'Random'),
	#('workload-8', 'ra8', 'Random 8'),
	#('workload-16', 'ra16', 'Random 16'),
]
renamed_results = {}

#for algorithm, runs in normalized_results.items():
for algorithm, runs in speedup_results.items():
	renamed_results[algorithm] = {}

	for run_name, results in runs.items():
		found = False
		for name, alias, title in aliases:
			i = 0
			for benchmarks in benchmark_sets[name]:
				if '-'.join(benchmarks) == run_name:
					renamed_results[algorithm]["%s-%02d" % (alias, i)] = results
					found = True
					break
				i = i + 1

			if found:
				break

	renamed_results[algorithm] = collections.OrderedDict(sorted(renamed_results[algorithm].items()))

print renamed_results

for metric in ['stp', 'antt']:
	for name, alias, title in aliases:
		fig,ax = plt.subplots()
		
		N = len([key for key,value in renamed_results['lru'].items() if key.startswith(alias+"-")])
		ind = np.arange(N)
		width = .11

		rects = []
		i = 0
		for algorithm in ['lru', 'tadip', 'drrip', 'ucp', 'pipp', 'pipp-nostream']:
			values = [value[metric] for key,value in renamed_results[algorithm].items() if key.startswith(alias+"-")]
			if len(values) != N:
				values = [0] * N
			rects.append(ax.bar(ind+(i*width), values, width, color=cm.Blues(1.*i/5)))
			i = i + 1


		#ax.set_ylabel("%s relative to LRU" % metric)
		ax.set_ylabel("%s" % metric)
		ax.set_xticks(ind+width*2.5)
		ax.set_xticklabels([ key for key,val in renamed_results['lru'].items() if key.startswith(alias+"-")], rotation=45)
		ax.legend((rects[0][0], rects[1][0], rects[2][0], rects[3][0], rects[4][0], rects[5][0]), ('lru', 'tadip', 'drrip', 'ucp', 'pipp', 'pipp-nostream'), fontsize='small', ncol=2)

		plt.suptitle(title)

		plt.savefig('%s/%s-%s.png' % (args.out, metric, alias), frameon=False, bbox_inches='tight')
		plt.clf()

