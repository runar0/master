#!/usr/bin/env python

import argparse
import os
import numpy as np
from matplotlib import pyplot as plt
from matplotlib import cm
import matplotlib
#matplotlib.rcParams.update({'font.size': 20})

from os import path
from sys import exit
import collections

from build import profiles
from benchmarks import benchmark_sets

def extract_ipc(filename, coreno = 0):
	instructions = extract_property('performance_model.instruction_count', filename, coreno)
	cycles = extract_property('performance_model.cycle_count', filename, coreno)
	if cycles == None: return 0.1
	return instructions/cycles

def extract_cycles(filename, coreno = 0):
	return extract_property('performance_model.cycle_count', filename, coreno)

def extract_property(property, filename, coreno = 0):
	try:
		with open(filename) as f:
			for line in f:
				line = line.split(' = ')
				if line[0] == property:
					return float(line[1].split(',')[coreno])
	except:
		return 0
	return 0

#print profiles

parser = argparse.ArgumentParser(description='Workload speedup/throughput grapher')
parser.add_argument('--dir', default='.', help='Experiment run directory')
parser.add_argument('--out', default='_figures/speedup/', help='Figure output directory')

args = parser.parse_args()

if not path.exists(args.dir):
	exit('Run directory %s does not exist!' % args.dir)

if not path.exists(args.out):
	os.makedirs(args.out)
	print 'Created output directory %s' % args.out

rundir = path.realpath(args.dir)

benchmark_results = { 'lru': {} }
workload_results = { 'lru': {}, 'tadip': {}, 'drrip': {}, 'ucp': {}, 'pipp': {}, 'pipp-nostream': {}, 'pipp-custom': {}, 'prism': {} }

for subdir, dirs, files in os.walk(rundir):
	for name in dirs:
		#if "l2-0128k" not in name: continue

		run = name.split('.')
		run_name = run[0]

		# Filter out invalid folders
		if run_name.startswith('_') or len(run) == 1:
			continue;

		benchmarks = run_name.split('-')
		algorithm = run[2]

		if len(benchmarks) == 1:
			benchmark_results[algorithm][run_name] = extract_cycles('%s/%s/stats-benchmark-0.txt' % (rundir, name))
		else:
			results = {}
			i = 0
			for benchmark in benchmarks:
				results[benchmark] = extract_cycles('%s/%s/stats-benchmark-%d.txt' % (rundir, name, i), i)
				i = i + 1
			workload_results[algorithm][run_name] = results

# Transform workload results to STP and HMS measurements
speedup_results = {}
for algorithm, runs in workload_results.items():
	speedup_results[algorithm] = {}

	for run_name, results in runs.items():
		stp = 0
		hms = 0

		for benchmark, cycles in results.items():
			hms = hms + cycles/benchmark_results['lru'][benchmark]
			stp = stp + benchmark_results['lru'][benchmark]/cycles if cycles > 0 else 0

		hms = len(results) / hms if hms > 0 else 0

		speedup_results[algorithm][run_name] = {
			'stp': stp,
			'hms': hms
		}

# convert run names to group names
aliases = [	
	('workload-cache', 'ca4', 'Cache'),
	('workload-cache-bw', 'cabw4', 'Cache-Bandwidth'),
	('workload-bw', 'bw4', 'Bandwidth'),
	('workload-compute', 'co4', 'Compute'),
	('workload-random', 'ra4', 'Random'),
	#('workload-8', 'ra8', 'Random 8'),
	#('workload-16', 'ra16', 'Random 16'),
]
renamed_results = {}

#for algorithm, runs in normalized_results.items():
for algorithm, runs in speedup_results.items():
	renamed_results[algorithm] = {}

	for run_name, results in runs.items():
		found = False
		for name, alias, title in aliases:
			i = 0
			for benchmarks in benchmark_sets[name]:
				if '-'.join(benchmarks) == run_name:
					renamed_results[algorithm]["%s-%02d" % (alias, i)] = results
					found = True
					break
				i = i + 1

			if found:
				break

	renamed_results[algorithm] = collections.OrderedDict(sorted(renamed_results[algorithm].items()))

for metric in ['stp', 'hms']:
	for name, alias, title in aliases:
		fig,ax = plt.subplots()
		
		N = len([key for key,value in renamed_results['lru'].items() if key.startswith(alias+"-")])
		ind = np.arange(N+1)
		width = .15

		rects = []
		i = 0
		for algorithm in ['lru', 'tadip', 'drrip', 'ucp', 'pipp', 'prism']:
			values = [value[metric] for key,value in renamed_results[algorithm].items() if key.startswith(alias+"-")]
			if len(values) != N:
				values = [0] * (N+1)
			else:
				values.append(np.mean(values))
			rects.append(ax.bar(ind+(i*width), values, width, color=cm.Blues(1.*i/5)))
			i = i + 1

		#ax.set_ylabel("%s relative to LRU" % metric)
		ax.set_ylabel("%s" % metric)
		ax.set_xticks(ind+width*2.5)
		ax.set_xticklabels([ key for key,val in renamed_results['lru'].items() if key.startswith(alias+"-")] + ['avg'], rotation=45)
		ax.legend((rects[0][0], rects[1][0], rects[2][0], rects[3][0], rects[4][0], rects[5][0]), ('lru', 'tadip', 'drrip', 'ucp', 'pipp', 'prism'), fontsize='small', ncol=3, bbox_to_anchor=(0.5, 2), loc='upper center')
		
		if metric == 'stp': ax.set_ylim([0,4.5])		
		if metric == 'hms': ax.set_ylim([0,1.2])
		ax.set_xlim([0,N+1])

		#plt.suptitle(title)
		fig.set_size_inches((5*np.floor(N/10), 2.5))
		plt.savefig('%s/%s-%s.png' % (args.out, metric, alias), frameon=False, bbox_inches='tight')
		plt.clf()


