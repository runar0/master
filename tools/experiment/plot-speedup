#!/usr/bin/env python

import argparse
import os
import numpy as np
from matplotlib import pyplot as plt
from matplotlib import cm
import matplotlib
#from scipy.stats import gmean
#matplotlib.rcParams.update({'font.size': 20})

import itertools
import pprint
pp = pprint.PrettyPrinter(indent=2)

from os import path
from sys import exit
import collections

from build import profiles
from benchmarks import benchmark_sets

def extract_results(filename, coreno):
	props = extract_properties({
			'performance_model.cycle_count': 'cycles', 
			'walltime': 'walltime', 
			'L3.load-misses': 'load_miss', 
			'L3.store-misses': 'store_miss', 
			'performance_model.instruction_count': 'instructions',
			'L3.stores': 'stores',
			'L3.loads': 'loads'
		}, filename, coreno)

	mpki = (props['load_miss'] + props['store_miss']) / (props['instructions']/1000) if props['instructions'] > 0 else 0

	return {
		'cycles': props['cycles'],
		'walltime': props['walltime'],
		'misses': props['load_miss'] + props['store_miss'],
		'mpki': mpki,
		'accesses': props['stores'] + props['loads']
	}

def extract_properties(properties, filename, coreno):
	results = {key: 0 for key in properties.values()}
	try:
		with open(filename) as f:
			result = {}
			for line in f:
				line = line.split(' = ')
				if line[0] in properties:
					values = line[1].split(',')
					if len(values) > 1:
						val = float(values[coreno])
					else:
						val = float(values[0])

					results[properties[line[0]]] = val

	except:
		print "Warning: Bad/Missing data", filename
		pass
	return results


# Extract walltime and cycle count for all benchmarks and workloads, return results as two separate dictionaries
def extract_raw_data(rundir):
	benchmark_results = {}
	workload_results = {}

	for subdir, dirs, files in os.walk(rundir):
		for name in dirs:
			run = name.replace('0.5M', '0_5M').split('.')
			run_name = run[0]

			# Filter out invalid folders
			if run_name.startswith('_') or len(run) == 1:
				continue;

			benchmarks = run_name.split('-')
			algorithm = run[2]
			l2size = [key.split('-')[1] for key in run if key.startswith("l2-")][0]
			csmb = [key.split('-')[1] for key in run if key.startswith("csmb-")][0]
			membus = [key.split('-')[1] for key in run if key.startswith("membus-")][0]
			l3size = [key.split('-')[1] for key in run if key.startswith("l3-")][0]

			if membus not in benchmark_results: benchmark_results[membus] = {}
			if membus not in workload_results: workload_results[membus] = {}
			if l3size not in benchmark_results[membus]: benchmark_results[membus][l3size] = {}
			if l3size not in workload_results[membus]: workload_results[membus][l3size] = {}
			if csmb not in benchmark_results[membus][l3size]: benchmark_results[membus][l3size][csmb] = {}
			if csmb not in workload_results[membus][l3size]: workload_results[membus][l3size][csmb] = {}
			if l2size not in benchmark_results[membus][l3size][csmb]: benchmark_results[membus][l3size][csmb][l2size] = {}
			if l2size not in workload_results[membus][l3size][csmb]: workload_results[membus][l3size][csmb][l2size] = {}


			if len(benchmarks) == 1:
				# use only 4M l3 configurations as baseline
				if not "l3-04M" in name: continue

				if algorithm not in benchmark_results[membus][l3size][csmb][l2size]: benchmark_results[membus][l3size][csmb][l2size][algorithm] = {}
				benchmark_results[membus][l3size][csmb][l2size][algorithm][run_name] = extract_results('%s/%s/stats-benchmark-0.txt' % (rundir, name), 0)['cycles']
			else:
				if algorithm not in workload_results[membus][l3size][csmb][l2size]: workload_results[membus][l3size][csmb][l2size][algorithm] = {}
				results = {}
				i = 0
				for benchmark in benchmarks:
					results[benchmark] = extract_results('%s/%s/stats-benchmark-%d.txt' % (rundir, name, i), i)
					i = i + 1
				workload_results[membus][l3size][csmb][l2size][algorithm][run_name] = results

	return (benchmark_results, workload_results)

# Given benchmark and workload results from extract_raw_data this function will calculate speedup data while keeping walltime
def transform_data_to_speedup(benchmark_results, workload_results):
	speedup_results = {}
	for membus in workload_results.keys():
		speedup_results[membus] = {}
		for l3size in workload_results[membus].keys():
			speedup_results[membus][l3size] = {}
			for csmb, runs in workload_results[membus][l3size].items():
				speedup_results[membus][l3size][csmb] = {}
				for l2size, algorithms in runs.items():
					speedup_results[membus][l3size][csmb][l2size] = {}
					for algorithm, runs in algorithms.items():
						speedup_results[membus][l3size][csmb][l2size][algorithm] = {}

						for run_name, results in runs.items():
							stp = 0
							hms = 0
							walltime = 0
							mpki = 0
							accesses = 0

							for benchmark, results in results.items():
								cycles = results['cycles']
								walltime = max(walltime, results['walltime'])
								mpki = max(mpki, results['mpki'])
								accesses = max(accesses, results['accesses'])

								hms = hms + cycles/benchmark_results['06_4']['04M'][csmb][l2size]['lru'][benchmark]
								stp = stp + benchmark_results['06_4']['04M'][csmb][l2size]['lru'][benchmark]/cycles if cycles > 0 else 0

							hms = len(results) / hms if hms > 0 else 0

							speedup_results[membus][l3size][csmb][l2size][algorithm][run_name] = {
								'stp': stp,
								'hms': hms,
								'walltime': walltime,
								'mpki': mpki,
								'accesses': accesses
							}
	return speedup_results

# Covert verbose run names to workload group specifiers, results are sorted by their position in the group
def transform_data_group_names(workload_results, aliases):
	renamed_results = {}

	for membus in workload_results.keys():
		renamed_results[membus] = {}
		for l3size in workload_results[membus].keys():
			renamed_results[membus][l3size] = {}
			for csmb, runs in workload_results[membus][l3size].items():

				renamed_results[membus][l3size][csmb] = {}
				for l2size, algorithms in runs.items():
					renamed_results[membus][l3size][csmb][l2size] = {}
					for algorithm, runs in algorithms.items():
						renamed_results[membus][l3size][csmb][l2size][algorithm] = {}

						for run_name, results in runs.items():
							found = False
							for name, alias, title in aliases:
								i = 0
								for benchmarks in benchmark_sets[name]:
									if '-'.join(benchmarks) == run_name:
										renamed_results[membus][l3size][csmb][l2size][algorithm]["%s-%02d" % (alias, i)] = results
										found = True
										break
									i = i + 1

								if found:
									break

						renamed_results[membus][l3size][csmb][l2size][algorithm] = collections.OrderedDict(sorted(renamed_results[membus][l3size][csmb][l2size][algorithm].items()))

	return renamed_results

# Loads all simulation data from the given run directory, transforms it into stp, hsm and walltime results and renames according to aliases and benchmark.py
def load_results(rundir, aliases):
	benchmark_results, workload_results = extract_raw_data(rundir);
	return transform_data_group_names(transform_data_to_speedup(benchmark_results, workload_results), aliases);

def plot_results(results, aliases, metrics, algorithms, rundir, yLimit = None, plotname = 'default', dropLRU = False, plotYLine = None, includeLegend = None, includeAverage=True, legendPos = None):
	fig,ax = plt.subplots()
	for csmb in results.keys():
		for l2size in results[csmb].keys():

			# Plot for each csmb-l2size combination
			overview = []
			for name, alias, title in aliases:

				# Hack to prevent the script from crashing on purposfully incomplete runs
				if len(results[csmb][l2size].keys()) != len(algorithms): 
					print "plot: ", plotname, " missing algorithms, skipping ", csmb, l2size
					continue

				# Find number of runs executed in this group
				N = len([key for key,value in results[csmb][l2size][algorithms[0]].items() if key.startswith(alias+"-")])

				if N == 0:
					#print results[csmb][l2size]['lru'].keys()
					print "No %s data for group %s (%s, csmb %s l2 %s), skipping." % (algorithms[0],title,name, csmb, l2size)
					continue 

				# Create indicies, cacluate bar width
				ind = np.arange(N+1) if includeAverage else np.arange(N)
				width = 1.0/(len(algorithms)+2)

				for metric in metrics:

					if plotYLine != None:
						plt.plot(range(N+2), [plotYLine] * (N+2), ':', color="black")


					# Build one bar plot per algorithm
					rects = []
					i = 1
					values = {}
					count_algorithms = len(algorithms) - (1 if dropLRU else 0)
					for algorithm in algorithms:
						if algorithm == "lru" and dropLRU: continue
						values[algorithm] = [value[metric] for key,value in results[csmb][l2size][algorithm].items() if key.startswith(alias+"-")]
						if len(values[algorithm]) != N:
							values[algorithm] = [0] * (N)
						
						if includeAverage:
							values[algorithm].append(np.mean(values[algorithm]))

						rects.append(ax.bar(ind+(i*width), values[algorithm], width*.8, color=cm.Blues(1.*i/count_algorithms), linewidth=0.45))
						i += 1

					ax.set_ylabel("%s" % metric)
					ax.set_xticks(ind+0.5)
					xlabels = [key.replace(alias+'-', '') for key,val in results[csmb][l2size][algorithms[0]].items() if key.startswith(alias+"-")] + (['avg'] if includeAverage else [])
					ax.set_xticklabels(xlabels, rotation=45)
					
					# Generate legends if requested
					if includeLegend != None and includeLegend(alias, metric):
						handles = ()
						legends = ()
						count = 0
						for algorithm in algorithms:
							if algorithm == "lru" and dropLRU: continue
							handles += (rects[count][0],)

							replacements = {'0_5M': '0.5MB', '01M': '1MB', '02M': '2MB', '04M': '4MB', '0128k': '128kB', '0256k': '256kB', '0512k': '512kB', '1024k': '1024kB', '01_6': '1.6GB/s', '03_2': '3.2GB/s', '06_4': '6.4GB/s'}
							if algorithm in replacements: algorithm = replacements[algorithm]
							legends += (algorithm,)
							count += 1

						if count <= 4:
							cols = 2
						else:
							cols = 2 if dropLRU else 3
						legendPos = (0.5, 1.45) if legendPos==None else legendPos
						ax.legend(handles, legends, fontsize='small', ncol=cols, bbox_to_anchor=legendPos, loc='upper center')


					if yLimit != None: ax.set_ylim(yLimit)
					ax.set_xlim([0,N+1]) if includeAverage else ax.set_xlim([0,N])

					fig.set_size_inches((2.5+1.25*np.floor(N/2.5), 2.5))
					plt.savefig('%s/%s-%s-%s-%s-%s.png' % (args.out, plotname, metric, l2size, csmb, alias), frameon=False, bbox_inches='tight')
					plt.cla()


					# Append data to overview
					if len(overview) == 0:
						overview.append(','.join([''] + [algorithm for algorithm in algorithms]))
					for i in xrange(N + (1 if includeAverage else 0)):
						if i < N:
							row = ['%s-%s-%02d' % (metric, alias, i)]
						else:
							row = ['%s-%s-avg' % (metric, alias)]

						for algorithm in algorithms:
							if algorithm == "lru" and dropLRU: continue
							row += [str(values[algorithm][i])]

						row = ','.join(row)
						overview.append(row)

			# Dump overview to a csv file
			with open('%s/%s-%s-%s.csv' % (args.out, plotname, l2size, csmb), "w") as file:
				file.write('\n'.join(overview))

# Generate HMS, STP and mpki results relative to LRU for a subset of the l2size and csmb configurations. 
def generate_speedup_results(results, l2size_filter = ['0128k'], csmb_filter = ['0100'], membus_filter=['06_4'], l3size_filter=['04M', '08M', '16M']):
	generated = {}
	for membus in results.keys():
		if membus not in membus_filter: continue
		for l3size in results[membus].keys():
			if l3size not in l3size_filter: continue
			for csmb, l2runs in results[membus][l3size].items():
				if csmb not in csmb_filter: continue
				if csmb not in generated: generated[csmb] = {}
				for l2size, algorithms in l2runs.items():
					if l2size not in l2size_filter: continue
					if l2size not in generated[csmb]: generated[csmb][l2size] = {}

					for algorithm, runs in algorithms.items():
						if algorithm not in generated[csmb][l2size]: generated[csmb][l2size][algorithm] = {}

						for alias, values in runs.items():
							if alias not in generated[csmb][l2size][algorithm]: generated[csmb][l2size][algorithm][alias] = {}
							for metric, value in values.items():
								# Store value relative to lru result
								generated[csmb][l2size][algorithm][alias][metric] = value/results[membus][l3size][csmb][l2size]["lru"][alias][metric]

				generated[csmb][l2size][algorithm] = collections.OrderedDict(sorted(generated[csmb][l2size][algorithm].items()))
	return generated


# Generate per workload size avarages for each csmb-l2size combination, NOTE: Input is the output from generate_speedup_results
def generate_size_averages(results):
	generated = {}
	# Generate the special avg-4, avg-8 and avg-16 runs
	for csmb, l2runs in results.items():
		generated[csmb] = {}
		for l2size, algorithms in l2runs.items():
			generated[csmb][l2size] = {}
			result = {}
			for algorithm, runs in algorithms.items():
				generated[csmb][l2size][algorithm] = {}

				averages = {key: {'hms': [], 'stp': [], 'mpki': []} for key in ['4', '8', '16']}
				fouraverages = {}
				for workload,values in runs.items():
					group = '4'
					if workload.startswith("ra8"): group = '8'
					elif workload.startswith("ra16"): group = '16'
					workload_group = workload.split('-')[0]
					
					if group == '4' and workload_group not in fouraverages:
						fouraverages[workload_group] = {'hms': [], 'stp': [], 'mpki': []};

					for metric in averages[group].keys():
						averages[group][metric].append(values[metric])
						if group == '4':
							fouraverages[workload_group][metric].append(values[metric])

				for cores in averages:
					vals = {}
					for metric in averages[cores]:
						vals[metric] = np.mean(averages[cores][metric]) if len(averages[cores][metric]) > 0 else 0
					generated[csmb][l2size][algorithm]['avg-%02d-core' % int(cores)] = vals
				for workload_group in fouraverages:
					vals = {}
					for metric in fouraverages[workload_group]:
						vals[metric] = np.mean(fouraverages[workload_group][metric]) if len(fouraverages[workload_group][metric]) > 0 else 0
					generated[csmb][l2size][algorithm]['4-avg-%s' % workload_group] = vals


				generated[csmb][l2size][algorithm] = collections.OrderedDict(sorted(generated[csmb][l2size][algorithm].items()))
	return generated

# Generate relative hms, stp and walltime comparisons accross csmb configurations
def generate_csmb_comparisons(results):
	generated = {}
	# Default csmb value and sorted list of all values in run
	default = '0100';
	csmbs = sorted([int(val) for val in results['06_4']['04M'].keys()])
	generated[default] = {}

	for membus in results.keys():
		if membus != '06_4': continue
		for l3size in results[membus].keys():
			if l3size not in ['04M']: continue
			for csmb in csmbs:
				csmb = '%04d' % csmb
				runs = results[membus][l3size][csmb]

				for l2size, algorithms in runs.items():
					if l2size != "0128k": continue

					if l2size not in generated[default]: generated[default][l2size] = {}
					result = {}
					for algorithm, runs in algorithms.items():
						if algorithm not in generated[default][l2size]: generated[default][l2size][algorithm] = collections.OrderedDict()

						averages = {key: {'hms': [], 'stp': [], 'walltime': [], 'mpki': []} for key in ['4', '8', '16']}

						for workload,values in runs.items():
							group = '4'
							if workload.startswith("ra8"): group = '8'
							elif workload.startswith("ra16"): group = '16'
							
							for metric in averages[group].keys():
								averages[group][metric].append(values[metric]/results[membus][l3size][default][l2size][algorithm][workload][metric])

						
						# Reduce list to averages and update generated
						for cores in averages:
							vals = {}
							for metric in averages[cores]:
								vals[metric] = np.mean(averages[cores][metric]) if len(averages[cores][metric]) > 0 else 0
							generated[default][l2size][algorithm]['csmb-%s-%s' % (cores, csmb)] = vals

	return generated

# Generate an overview of average change in l3 access for all lru-cmsb-100 runs, used to highlight how l2 size filters l3 accesses
def generate_l3_access_averages(results):
	generated = {}
	count = {'4': 0, '8': 0, '16': 0}
	# Generate the special avg-4, avg-8 and avg-16 runs
	for membus in results.keys():
		if membus != '06_4': continue
		for l3size in results[membus].keys():
			if l3size not in ['04M', '08M', '16M']: continue
			for csmb, l2runs in results[membus][l3size].items():
				if csmb != '0100': continue

				if csmb not in generated: generated[csmb] = {'0128k': {}}
				for l2size, algorithms in l2runs.items():
					result = {}
					for algorithm, runs in algorithms.items():
						if algorithm != "lru": continue

						if l2size not in generated[csmb]['0128k']: generated[csmb]['0128k'][l2size] = {}

						averages = {key: {'accesses': []} for key in ['4', '8', '16']}
						fouraverages = {}
						for workload,values in runs.items():
							random = workload.startswith("ra")
							group = '4'
							if workload.startswith("ra8"): group = '8'
							elif workload.startswith("ra16"): group = '16'
							workload_group = workload.split('-')[0]
							
							if group == '4' and workload_group not in fouraverages:
								fouraverages[workload_group] = {'accesses': []};

							for metric in averages[group].keys():
								if values[metric] == 0:
									count[group] += 1
								else:
									if random: averages[group][metric].append(values[metric]/results[membus][l3size][csmb]["0128k"][algorithm][workload][metric])
									#if random: averages[group][metric].append(values[metric])
									if group == '4':
										fouraverages[workload_group][metric].append(values[metric]/results[membus][l3size][csmb]["0128k"][algorithm][workload][metric])
										#fouraverages[workload_group][metric].append(values[metric])

						for cores in averages:
							if len(averages[cores]['accesses']) > 0:
								vals = {}
								for metric in averages[cores]:
									vals[metric] = np.mean(averages[cores][metric]) if len(averages[cores][metric]) > 0 else 0
								generated[csmb]['0128k'][l2size]['access-ra%02d' % int(cores)] = vals
						for workload_group in fouraverages:
							if len(fouraverages[workload_group]['accesses']) > 0:
								vals = {}
								for metric in fouraverages[workload_group]:
									vals[metric] = np.mean(fouraverages[workload_group][metric]) if len(fouraverages[workload_group][metric]) > 0 else 0
								generated[csmb]['0128k'][l2size]['4-access-%s' % workload_group] = vals


						generated[csmb]['0128k'][l2size] = collections.OrderedDict(sorted(generated[csmb]['0128k'][l2size].items()))

	print "L3 access health: ", count
	return generated

def generate_l2size_comparison(results):
	generated = {}
	# Default l2size value and sorted list of all values in run
	default = '0128k';
	l2sizes = sorted([val for val in results['06_4']['04M']['0100'].keys()])
	generated = {}

	csmb = '%04d' % 100

	count = {'4': 0, '8': 0, '16': 0}

	for membus in results.keys():
		if membus != "06_4": continue
		for l3size in results[membus].keys():
			if l3size not in ['04M', '08M', '16M']: continue

			l2runs = results[membus][l3size][csmb]
			for l2size in l2sizes:
				algorithms = l2runs[l2size]
				
				result = {}
				for algorithm, runs in algorithms.items():
					if algorithm not in generated: generated[algorithm] = {default: {}}
					if l2size not in generated[algorithm][default]: generated[algorithm][default][l2size] = {}

					averages = {key: {'hms': [], 'stp': [], 'walltime': [], 'mpki': []} for key in ['4', '8', '16']}

					for workload,values in runs.items():
						group = '4'
						if workload.startswith("ra8"): group = '8'
						elif workload.startswith("ra16"): group = '16'
						
						for metric in averages[group].keys():
							if values[metric] == 0:
								count[group] += 1
							else:
								averages[group][metric].append(values[metric]/results[membus][l3size]['0100'][l2size]["lru"][workload][metric])

					
					# Reduce list to averages and update generated
					for cores in averages:
						if len(averages[cores]['stp']) > 0:								
							vals = {}
							for metric in averages[cores]:
								vals[metric] = np.mean(averages[cores][metric]) if len(averages[cores][metric]) > 0 else 0
							generated[algorithm][default][l2size]['l2-ra%02d' % (int(cores))] = vals

					generated[algorithm][default][l2size] = collections.OrderedDict(sorted(generated[algorithm][default][l2size].items()))

	print "L2 comp health: ", count
	return generated

def generate_l3size_comparison(results):
	# Default l3size value and sorted list of all values in run
	default = '04M';
	l3sizes = sorted(['0_5M', '01M', '02M', '04M'])
	generated = {'all': {default: {}}}

	csmb = '%04d' % 100

	for membus in results.keys():
		if membus != "06_4": continue
		for l3size in l3sizes:
			algorithms = results[membus][l3size][csmb]['0128k']
			
			result = {}
			for algorithm, runs in algorithms.items():
				if algorithm not in generated: generated[algorithm] = {default: {}}
				if l3size not in generated[algorithm][default]: generated[algorithm][default][l3size] = {}
				if l3size not in generated['all'][default]: generated['all'][default][l3size] = {}

				averages = {key: {'hms': [], 'stp': [], 'walltime': [], 'mpki': []} for key in ['4', '8', '16']}

				for workload,values in runs.items():
					group = '4'
					if workload.startswith("ra8"): group = '8'
					elif workload.startswith("ra16"): group = '16'
					
					for metric in averages[group].keys():
						averages[group][metric].append(values[metric]/results[membus][l3size]['0100']['0128k']["lru"][workload][metric])

				
				# Reduce list to averages and update generated
				for cores in averages:
					if len(averages[cores]['stp']) > 0:								
						vals = {}
						for metric in averages[cores]:
							vals[metric] = np.mean(averages[cores][metric]) if len(averages[cores][metric]) > 0 else 0
						generated[algorithm][default][l3size]['l3-%s' % algorithm] = vals
						if algorithm != 'lru': generated['all'][default][l3size]['l3-%s' % algorithm] = vals

				generated[algorithm][default][l3size] = collections.OrderedDict(sorted(generated[algorithm][default][l3size].items()))
				generated['all'][default][l3size] = collections.OrderedDict(sorted(generated['all'][default][l3size].items()))

	return generated

def generate_membus_comparison(results, l3filter = ['04M']):
	# Default l2size value and sorted list of all values in run
	default = '06_4';
	membuses = sorted(['01_6', '03_2', '06_4'])
	generated = {'all': {default: {}}}

	csmb = '%04d' % 100

	for membus in membuses:
		for l3size in results[membus].keys():
			if l3size not in l3filter: continue

			l2runs = results[membus][l3size][csmb]
			for l2size in ['0128k']:
				algorithms = l2runs[l2size]
				
				result = {}
				for algorithm, runs in algorithms.items():
					if algorithm not in generated: generated[algorithm] = {default: {}}
					if membus not in generated[algorithm][default]: generated[algorithm][default][membus] = {}
					if membus not in generated['all'][default]: generated['all'][default][membus] = {}

					averages = {key: {'hms': [], 'stp': [], 'walltime': [], 'mpki': []} for key in ['4', '8', '16']}

					for workload,values in runs.items():
						group = '4'
						if workload.startswith("ra8"): group = '8'
						elif workload.startswith("ra16"): group = '16'
						
						for metric in averages[group].keys():
							averages[group][metric].append(values[metric]/results[membus][l3size]['0100'][l2size]["lru"][workload][metric])

					
					# Reduce list to averages and update generated
					for cores in averages:
						if len(averages[cores]['stp']) > 0:								
							vals = {}
							for metric in averages[cores]:
								vals[metric] = np.mean(averages[cores][metric]) if len(averages[cores][metric]) > 0 else 0
							generated[algorithm][default][membus]['membus-%s' % algorithm] = vals
							if algorithm != 'lru': generated['all'][default][membus]['membus-%s' % algorithm] = vals

					generated[algorithm][default][membus] = collections.OrderedDict(sorted(generated[algorithm][default][membus].items()))
					generated['all'][default][membus] = collections.OrderedDict(sorted(generated['all'][default][membus].items()))

	return generated

def generate_pubyear_comparison(results):

	generated = {}
	for l3size in results['06_4'].keys():
		if l3size not in ['04M', '08M', '16M']: continue
		for algorithm, runs in results['06_4'][l3size]['0100']['0128k'].items():
			if algorithm == "lru" or algorithm == "pipp-min8": continue
			if algorithm not in generated: generated[algorithm] = {}

			averages = {key: {'hms': [], 'stp': [], 'mpki': []} for key in ['4', '8', '16']}

			for workload,values in runs.items():
				# Skip non random workloads
				if not workload.startswith("ra"): continue

				group = '4'
				if workload.startswith("ra8"): group = '8'
				elif workload.startswith("ra16"): group = '16'
				
				for metric in averages[group].keys():
					averages[group][metric].append(values[metric]/results['06_4'][l3size]['0100']['0128k']["lru"][workload][metric])

			
			# Reduce list to averages and update generated
			for cores in averages:
				if len(averages[cores]['stp']) > 0:
					vals = {}
					for metric in averages[cores]:
						vals[metric] = np.mean(averages[cores][metric]) if len(averages[cores][metric]) > 0 else 0
					generated[algorithm][cores] = vals

	return generated

parser = argparse.ArgumentParser(description='Workload speedup/throughput grapher')
parser.add_argument('--dir', default='.', help='Experiment run directory')
parser.add_argument('--out', default='_figures/speedup/', help='Figure output directory')

args = parser.parse_args()

if not path.exists(args.dir):
	exit('Run directory %s does not exist!' % args.dir)

if not path.exists(args.out):
	os.makedirs(args.out)
	print 'Created output directory %s' % args.out

rundir = path.realpath(args.dir)

# Mapping between workload names in benchmarks.py (ex workload-cache) and the report name (ex ca4)
aliases = [
	('workload-cache', 'ca4', 'Cache'),
	('workload-cache-bw', 'cabw4', 'Cache-Bandwidth'),
	('workload-bw', 'bw4', 'Bandwidth'),
	('workload-compute', 'co4', 'Compute'),
	('workload-random', 'ra4', 'Random'),
	('workload-8', 'ra8', 'Random 8'),
	('workload-16', 'ra16', 'Random 16'),
]

# Partition algorithms to plot
algorithms = ['lru', 'tadip', 'drrip-3', 'ucp', 'pipp', 'pipp-min8', 'prism']

genSpeedup = True
genCsmbComp = False
genL2Comp = True
genL3Comp = True
genPubYearComp = False

results = load_results(rundir, aliases);

# Plot speedup result and average stp and hms plots for base experiment
if genSpeedup:
	legend = lambda alias, metric: alias.startswith('ra')
	speedup = generate_speedup_results(results)
	plot_results(speedup, aliases, ['stp', 'hms'], algorithms, rundir, plotname='speedup', dropLRU=True, yLimit=[0.8,1.1], plotYLine=1, includeLegend=lambda alias, metric: metric == "stp")
	plot_results(speedup, aliases, ['mpki'], algorithms, rundir, plotname='speedup', dropLRU=True, yLimit=[0.9,1.6], plotYLine=1, includeLegend=lambda alias, metric: metric == "stp")
	avg = generate_size_averages(speedup)
	plot_results(avg, [('', 'avg', ''), ('', '4-avg', '')], ['stp', 'hms'], algorithms, rundir, yLimit=[0.8,1.1], includeAverage=False, plotname='avg', dropLRU=True, plotYLine=1, includeLegend=lambda alias, metric: (alias == 'avg' and metric=='mpki') or (alias == '4-avg' and metric=='stp'))
	plot_results(avg, [('', 'avg', ''), ('', '4-avg', '')], ['mpki'], algorithms, rundir, yLimit=[0.9,1.55], includeAverage=False, plotname='avg', dropLRU=True, plotYLine=1, includeLegend=lambda alias, metric: (alias == 'avg' and metric=='mpki') or (alias == '4-avg' and metric=='stp'))
	speedup = None
	avg = None

# Plot csmb comparisons
if genCsmbComp:
	csmb_results = generate_csmb_comparisons(results)
	plot_results(csmb_results, [('', 'csmb-4', '')], ['stp', 'hms', 'mpki'], algorithms, rundir, plotname='csmb', yLimit=[0.98,1.01], dropLRU=False, includeAverage=False)
	plot_results(csmb_results, [('', 'csmb-4', '')], ['walltime'], algorithms, rundir, plotname='csmb', yLimit=[0,3], includeLegend=lambda alias, metric: True, dropLRU=False, includeAverage=False)

# Plot l2 comparisons
if genL2Comp:
	plot_results(generate_l3_access_averages(results), [('', 'access', ''), ('', '4-access', '')], ['accesses'], ['0128k', '0256k', '0512k', '1024k'], rundir, plotname='accesses', yLimit=[0.85,1.0], dropLRU=False, includeAverage=False, includeLegend=lambda a,b: True)
	plot_results(generate_l2size_comparison(results), [('', 'l2', '')], ['stp', 'hms', 'mpki'], ['0128k', '0256k', '0512k', '1024k'], rundir, plotname='l2', yLimit=[0.50,1.15], dropLRU=False, includeAverage=False, includeLegend=lambda a,b: False)
	plot_results(generate_l2size_comparison(results), [('', 'l2', '')], ['stp', 'hms', 'mpki'], ['0128k', '0256k', '0512k', '1024k'], rundir, plotname='l2-legend', yLimit=[0.50,1.15], dropLRU=False, includeAverage=False, includeLegend=lambda a,b: True)

if genL3Comp:
	generated = generate_l3size_comparison(results)
	plot_results(generated, [('', 'l3', '')], ['stp', 'hms'], ['0_5M', '01M', '02M', '04M'], rundir, plotname='l3', yLimit=[0.9,1.05], dropLRU=False, includeAverage=False, includeLegend=lambda a,b: True,  legendPos=(0.5, 1.35))
	plot_results(generated, [('', 'l3', '')], ['mpki'], ['0_5M', '01M', '02M', '04M'], rundir, plotname='l3', yLimit=[0.85,1.30], dropLRU=False, includeAverage=False, includeLegend=lambda a,b: False)
	plot_results(generated, [('', 'l3', '')], ['mpki'], ['0_5M', '01M', '02M', '04M'], rundir, plotname='l3-legend', yLimit=[0.85,1.30], dropLRU=False, includeAverage=False, includeLegend=lambda a,b: True, legendPos=(0.5, 1.35))

	generated = generate_membus_comparison(results, l3filter=['04M'])
	plot_results(generated, [('', 'membus', '')], ['stp', 'hms'], ['01_6', '03_2', '06_4'], rundir, plotname='membus', yLimit=[0.85,1.10], dropLRU=False, includeAverage=False, includeLegend=lambda a,b: False)
	plot_results(generated, [('', 'membus', '')], ['stp', 'hms'], ['01_6', '03_2', '06_4'], rundir, plotname='membus-legend', yLimit=[0.85,1.10], dropLRU=False, includeAverage=False, includeLegend=lambda a,b: True, legendPos=(0.5, 1.35))
	plot_results(generated, [('', 'membus', '')], ['mpki'], ['01_6', '03_2', '06_4'], rundir, plotname='membus', yLimit=[0.85,1.30], dropLRU=False, includeAverage=False, includeLegend=lambda a,b: False)

	generated = generate_membus_comparison(results, l3filter=['0_5M'])
	plot_results(generated, [('', 'membus', '')], ['stp', 'hms'], ['01_6', '03_2', '06_4'], rundir, plotname='membus-0_5M', yLimit=[0.85,1.25], dropLRU=False, includeAverage=False, includeLegend=lambda a,b: False)
	plot_results(generated, [('', 'membus', '')], ['stp', 'hms'], ['01_6', '03_2', '06_4'], rundir, plotname='membus-0_5M-legend', yLimit=[0.85,1.25], dropLRU=False, includeAverage=False, includeLegend=lambda a,b: True, legendPos=(0.5, 1.35))
	plot_results(generated, [('', 'membus', '')], ['mpki'], ['01_6', '03_2', '06_4'], rundir, plotname='membus-0_5M', yLimit=[0.85,1.30], dropLRU=False, includeAverage=False, includeLegend=lambda a,b: False)

if genPubYearComp:
	years = {
		'tadip': 2008,
		'drrip-3': 2010,
		'ucp': 2006,
		'pipp': 2009,
		'prism': 2012
	}
	result = generate_pubyear_comparison(results)

	stp = [(years[algorithm], values['4']['stp']) for algorithm,values in result.items()]
  	stp = [list(t) for t in zip(*stp)]
  	hms = [(years[algorithm], values['4']['hms']) for algorithm,values in result.items()]
  	hms = [list(t) for t in zip(*hms)]

  	fig,ax = plt.subplots()
	stp_handle = plt.plot(stp[0], stp[1], 'ro', color=cm.Blues(0.25))
	hms_handle = plt.plot(hms[0], hms[1], 'ro', color=cm.Blues(0.75))
	plt.legend([stp_handle[0], hms_handle[0]], ['stp', 'hms'], ncol=2)
	ax = plt.gca()
	ax.get_xaxis().get_major_formatter().set_useOffset(False)
	plt.axis([2005, 2013, 0.95, 1.1])
	ax.set_xticks([2006, 2008, 2009, 2010, 2012])
	ax.set_ylabel("STP/HMS normalized to LRU")
	ax.set_xticklabels(['ucp\n(2006)', 'tadip\n(2008)', 'pipp\n(2009)', 'drrip\n(2010)', 'prism\n(2012)'], rotation=45)

	fig.set_size_inches((2.5+5.0, 2.5))
	plt.savefig('%s/year-comp.png' % (args.out), frameon=False, bbox_inches='tight')
	plt.cla()
