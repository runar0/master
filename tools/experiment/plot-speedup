#!/usr/bin/env python

import argparse
import os
import numpy as np
from matplotlib import pyplot as plt
from matplotlib import cm
import matplotlib
from scipy.stats import gmean
#matplotlib.rcParams.update({'font.size': 20})

import itertools
import pprint
pp = pprint.PrettyPrinter(indent=2)

from os import path
from sys import exit
import collections

from build import profiles
from benchmarks import benchmark_sets

def extract_ipc(filename, coreno = 0):
	instructions = extract_property('performance_model.instruction_count', filename, coreno)
	cycles = extract_property('performance_model.cycle_count', filename, coreno)
	if cycles == None: return 0.1
	return instructions/cycles

def extract_cycles(filename, coreno = 0):
	return extract_property('performance_model.cycle_count', filename, coreno)


def extract_walltime(filename, coreno = 0):
	return extract_property('walltime', filename, coreno)

def extract_property(property, filename, coreno = 0):
	try:
		with open(filename) as f:
			for line in f:
				line = line.split(' = ')
				if line[0] == property:
					return float(line[1].split(',')[coreno])
	except:
		return 0
	return 0

#print profiles

parser = argparse.ArgumentParser(description='Workload speedup/throughput grapher')
parser.add_argument('--dir', default='.', help='Experiment run directory')
parser.add_argument('--out', default='_figures/speedup/', help='Figure output directory')

args = parser.parse_args()

if not path.exists(args.dir):
	exit('Run directory %s does not exist!' % args.dir)

if not path.exists(args.out):
	os.makedirs(args.out)
	print 'Created output directory %s' % args.out

rundir = path.realpath(args.dir)

benchmark_results = {}
workload_results = {}

for subdir, dirs, files in os.walk(rundir):
	for name in dirs:
		run = name.split('.')
		run_name = run[0]

		# Filter out invalid folders
		if run_name.startswith('_') or len(run) == 1:
			continue;

		benchmarks = run_name.split('-')
		algorithm = run[2]
		l2size = [key.split('-')[1] for key in run if key.startswith("l2-")][0]
		csmb = [key.split('-')[1] for key in run if key.startswith("csmb-")][0]
		if csmb == "100": csmb = "0100"

		if csmb not in benchmark_results: benchmark_results[csmb] = {}
		if csmb not in workload_results: workload_results[csmb] = {}
		if l2size not in benchmark_results[csmb]: benchmark_results[csmb][l2size] = {}
		if l2size not in workload_results[csmb]: workload_results[csmb][l2size] = {}


		if len(benchmarks) == 1:
			# use only 4M l3 configurations as baseline
			if not "l3-04M" in name: continue

			if algorithm not in benchmark_results[csmb][l2size]: benchmark_results[csmb][l2size][algorithm] = {}
			benchmark_results[csmb][l2size][algorithm][run_name] = extract_cycles('%s/%s/stats-benchmark-0.txt' % (rundir, name))
		else:
			if algorithm not in workload_results[csmb][l2size]: workload_results[csmb][l2size][algorithm] = {}
			results = {}
			i = 0
			for benchmark in benchmarks:
				results[benchmark] = {
						'cycles': extract_cycles('%s/%s/stats-benchmark-%d.txt' % (rundir, name, i), i),
						'walltime': extract_walltime('%s/%s/stats-benchmark-%d.txt' % (rundir, name, i), i)
					}
				i = i + 1
			workload_results[csmb][l2size][algorithm][run_name] = results

# Transform workload results to STP and HMS measurements
speedup_results = {}
for csmb, runs in workload_results.items():
	speedup_results[csmb] = {}
	for l2size, algorithms in runs.items():
		speedup_results[csmb][l2size] = {}
		for algorithm, runs in algorithms.items():
			speedup_results[csmb][l2size][algorithm] = {}

			for run_name, results in runs.items():
				stp = 0
				hms = 0
				walltime = 0

				for benchmark, results in results.items():
					cycles = results['cycles']
					walltime = max(walltime, results['walltime'])

					hms = hms + cycles/benchmark_results[csmb][l2size]['lru'][benchmark]
					stp = stp + benchmark_results[csmb][l2size]['lru'][benchmark]/cycles if cycles > 0 else 0

				hms = len(results) / hms if hms > 0 else 0

				speedup_results[csmb][l2size][algorithm][run_name] = {
					'stp': stp,
					'hms': hms,
					'walltime': walltime
			}

# Results are now in speedup_results
benchmark_results = None
workload_results = None

# convert run names to group names
aliases = [
	('workload-cache', 'ca4', 'Cache'),
	('workload-cache-bw', 'cabw4', 'Cache-Bandwidth'),
	('workload-bw', 'bw4', 'Bandwidth'),
	('workload-compute', 'co4', 'Compute'),
	('workload-random', 'ra4', 'Random'),
	('workload-8', 'ra8', 'Random 8'),
	('workload-16', 'ra16', 'Random 16'),

	# Fake group names, for dervied results generated below
	('overall-avg', '-avg', 'Overall Average'),
	('csmb-comp', '-csmb-comp-4', '4-core CSMB sensitivity comparison')
]
# Partition algorithms to plot
algorithms_names = ['lru', 'tadip', 'drrip-3', 'ucp', 'pipp', 'pipp-min8', 'prism']
for algorithm in algorithms_names:
	for csmb in speedup_results.keys():
		for l2size in speedup_results[csmb].keys():
			# A incomplete run will skrew complete runs with different l2 size, stupid, but the easiest way of acheiving stability
			if algorithm not in speedup_results[csmb][l2size]:				
				print "Missing results for algorithm %s (l2 %s), removing this algorithm from plot." % (algorithm, l2size)
				algorithms_names.remove(algorithm)

renamed_results = {}

for csmb, runs in speedup_results.items():

	renamed_results[csmb] = {}
	for l2size, algorithms in runs.items():
		renamed_results[csmb][l2size] = {}
		for algorithm, runs in algorithms.items():
			renamed_results[csmb][l2size][algorithm] = {}

			for run_name, results in runs.items():
				found = False
				for name, alias, title in aliases:
					i = 0
					for benchmarks in benchmark_sets[name]:
						if '-'.join(benchmarks) == run_name:
							renamed_results[csmb][l2size][algorithm]["%s-%02d" % (alias, i)] = results
							found = True
							break
						i = i + 1

					if found:
						break

			renamed_results[csmb][l2size][algorithm] = collections.OrderedDict(sorted(renamed_results[csmb][l2size][algorithm].items()))

# Speedup_result is now in renamed_results
speedup_results = None


# Generate the special avg-4, avg-8 and avg-16 runs
for csmb, runs in renamed_results.items():
	for l2size, algorithms in runs.items():
		result = {}
		for algorithm, runs in algorithms.items():
			avrages = {'4': {'hms': [], 'stp': []}, '8': {'hms': [], 'stp': []}, '16': {'hms': [], 'stp': []}}
			for workload,values in runs.items():
				if workload.startswith('-'): continue
				group = '4'
				if workload.startswith("ra8"): group = '8'
				elif workload.startswith("ra16"): group = '16'
				
				avrages[group]['stp'].append(values['stp'])
				avrages[group]['hms'].append(values['hms'])
			
			for cores in ['4', '8', '16']:
				renamed_results[csmb][l2size][algorithm]['-avg-%s' % cores] = {
					'hms': 0, 'stp': 0, 'walltime': 0
				};
				if len(avrages[cores]['stp']) > 0:
					renamed_results[csmb][l2size][algorithm]['-avg-%s' % cores] = {
						'hms': gmean(avrages[cores]['hms']),
						'stp': gmean(avrages[cores]['stp']) / (float(cores)),
						'walltime': 0
					}

# Generate the special csmb-comp-4-*, csmb-comp-8-* and csmb-comp-16-* runs
csmbs = sorted([int(val) for val in renamed_results.keys()])
default = '0100';
for csmb in csmbs:
	csmb = '%04d' % csmb
	runs = renamed_results[csmb]
	for l2size, algorithms in runs.items():
		result = {}
		for algorithm, runs in algorithms.items():
			avrages = {'4': {'hms': [], 'stp': [], 'walltime': []}}
			for workload,values in runs.items():
				if workload.startswith('-'): continue
				group = '4'
				if workload.startswith("ra8"): group = '8'
				elif workload.startswith("ra16"): group = '16'
				if group != '4': continue;
				
				avrages[group]['stp'].append(values['stp']/renamed_results[default][l2size][algorithm][workload]['stp'])
				avrages[group]['hms'].append(values['hms']/renamed_results[default][l2size][algorithm][workload]['hms'])
				avrages[group]['walltime'].append(values['walltime']/renamed_results[default][l2size][algorithm][workload]['walltime'])
			
			for cores in ['4']: #['4', '8', '16']:
				renamed_results[csmb][l2size][algorithm]['-csmb-comp-%s-%s' % (cores, csmb)] = {
					'hms': 0, 'stp': 0, 'walltime': 0
				};
				if len(avrages[cores]['stp']) > 0:
					renamed_results[default][l2size][algorithm]['-csmb-comp-%s-%s' % (cores, csmb)] = {
						'hms': gmean(avrages[cores]['hms']),
						'stp': gmean(avrages[cores]['stp']),
						'walltime': gmean(avrages[cores]['walltime'])
					}


# Finally plot all graphs
for csmb in renamed_results.keys():
	for l2size in renamed_results[csmb].keys():
		overview = []
		for name, alias, title in aliases:
			# Find number of runs executed in this group
			N = len([key for key,value in renamed_results[csmb][l2size]['lru'].items() if key.startswith(alias+"-")])

			if N == 0:
				print "No lru data for group %s (%s, csmb %s l2 %s), skipping." % (title,name, csmb, l2size)
				continue 

			# Create indicies, cacluate bar width
			ind = np.arange(N+1)
			width = 1.0/(len(algorithms_names)+2)

			for metric in ['stp', 'hms', 'walltime']:
				fig,ax = plt.subplots()

				# Build one bar plot per algorithm
				rects = []
				i = 1
				values = {}
				for algorithm in algorithms_names:
					values[algorithm] = [value[metric] for key,value in renamed_results[csmb][l2size][algorithm].items() if key.startswith(alias+"-")]
					if len(values[algorithm]) != N:
						values[algorithm] = [0] * (N+1)
					else:
						values[algorithm].append(gmean(values[algorithm]))

					rects.append(ax.bar(ind+(i*width), values[algorithm], width*.8, color=cm.Blues(1.*i/len(algorithms_names)), linewidth=0.45))
					i += 1

				ax.set_ylabel("%s" % metric)
				ax.set_xticks(ind+0.5)
				ax.set_xticklabels([key for key,val in renamed_results[csmb][l2size]['lru'].items() if key.startswith(alias+"-")] + ['avg'], rotation=45)
				
				# Generate legends, but only for the ra images
				if alias == "ra4" or (metric == "stp" and alias.startswith("ra")):
					handles = ()
					legends = ()
					count = 0
					for algorithm in algorithms_names:
						handles += (rects[count][0],)
						legends += (algorithm,)
						count += 1
					ax.legend(handles, legends, fontsize='small', ncol=3, bbox_to_anchor=(0.5, 1.40), loc='upper center')
				

				if metric == 'stp': 
					if alias == 'ra8':
						ax.set_ylim([2,8])		
					elif alias == 'ra16':
						ax.set_ylim([2,16])		
					elif alias == '-avg':
						ax.set_ylim([0,1])
					elif alias.startswith('-csmb-comp'):
						ax.set_ylim([0.9,1.1])
					else:
						ax.set_ylim([1,4])		
				if metric == 'hms': ax.set_ylim([0,1])
				ax.set_xlim([0,N+1])

				fig.set_size_inches((2.5+2.5*np.floor(N/5), 2.5))
				plt.savefig('%s/%s-%s-%s-%s.png' % (args.out, metric, l2size, csmb, alias), frameon=False, bbox_inches='tight')
				plt.clf()


				# Append data to overview
				if len(overview) == 0:
					overview.append(','.join([''] + [algorithm + ',' for algorithm in algorithms_names]))
				for i in xrange(N+1):
					if i < N:
						row = ['%s-%s-%02d' % (metric, alias, i)]
					else:
						row = ['%s-%s-avg' % (metric, alias)]

					for algorithm in algorithms_names:
						if values['lru'][i] > 0:
							row += [str(values[algorithm][i]), str((values[algorithm][i]-values['lru'][i])/values['lru'][i])]
						else:
							row += [str(values[algorithm][i]),'0']

					row = ','.join(row)
					overview.append(row)

		# Dump overview to a csv file
		with open('%s/%s-%s.csv' % (args.out, l2size, csmb), "w") as file:
			file.write('\n'.join(overview))