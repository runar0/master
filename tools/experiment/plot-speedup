#!/usr/bin/env python

import argparse
import os
import numpy as np
from matplotlib import pyplot as plt
from matplotlib import cm
import matplotlib
#from scipy.stats import gmean
#matplotlib.rcParams.update({'font.size': 20})

import itertools
import pprint
pp = pprint.PrettyPrinter(indent=2)

from os import path
from sys import exit
import collections

from build import profiles
from benchmarks import benchmark_sets

def extract_results(filename, coreno):
	props = extract_properties({
			'performance_model.cycle_count': 'cycles', 
			'walltime': 'walltime', 
			'L3.load-misses': 'load_miss', 
			'L3.store-misses': 'store_miss', 
			'performance_model.instruction_count': 'instructions'
		}, filename, coreno)

	mpki = (props['load_miss'] + props['store_miss']) / (props['instructions']/1000) if props['instructions'] > 0 else 0

	return {
		'cycles': props['cycles'],
		'walltime': props['walltime'],
		'misses': props['load_miss'] + props['store_miss'],
		'mpki': mpki
	}

def extract_properties(properties, filename, coreno):
	results = {key: 0 for key in properties.values()}
	try:
		with open(filename) as f:
			result = {}
			for line in f:
				line = line.split(' = ')
				if line[0] in properties:
					values = line[1].split(',')
					if len(values) > 1:
						val = float(values[coreno])
					else:
						val = float(values[0])

					results[properties[line[0]]] = val

	except:
		pass
	return results


# Extract walltime and cycle count for all benchmarks and workloads, return results as two separate dictionaries
def extract_raw_data(rundir):
	benchmark_results = {}
	workload_results = {}

	for subdir, dirs, files in os.walk(rundir):
		for name in dirs:
			run = name.split('.')
			run_name = run[0]

			# Filter out invalid folders
			if run_name.startswith('_') or len(run) == 1:
				continue;

			benchmarks = run_name.split('-')
			algorithm = run[2]
			l2size = [key.split('-')[1] for key in run if key.startswith("l2-")][0]
			csmb = [key.split('-')[1] for key in run if key.startswith("csmb-")][0]

			if csmb not in benchmark_results: benchmark_results[csmb] = {}
			if csmb not in workload_results: workload_results[csmb] = {}
			if l2size not in benchmark_results[csmb]: benchmark_results[csmb][l2size] = {}
			if l2size not in workload_results[csmb]: workload_results[csmb][l2size] = {}


			if len(benchmarks) == 1:
				# use only 4M l3 configurations as baseline
				if not "l3-04M" in name: continue

				if algorithm not in benchmark_results[csmb][l2size]: benchmark_results[csmb][l2size][algorithm] = {}
				benchmark_results[csmb][l2size][algorithm][run_name] = extract_results('%s/%s/stats-benchmark-0.txt' % (rundir, name), 0)['cycles']
			else:
				if algorithm not in workload_results[csmb][l2size]: workload_results[csmb][l2size][algorithm] = {}
				results = {}
				i = 0
				for benchmark in benchmarks:
					results[benchmark] = extract_results('%s/%s/stats-benchmark-%d.txt' % (rundir, name, i), i)
					i = i + 1
				workload_results[csmb][l2size][algorithm][run_name] = results

	return (benchmark_results, workload_results)

# Given benchmark and workload results from extract_raw_data this function will calculate speedup data while keeping walltime
def transform_data_to_speedup(benchmark_results, workload_results):
	speedup_results = {}
	for csmb, runs in workload_results.items():
		speedup_results[csmb] = {}
		for l2size, algorithms in runs.items():
			speedup_results[csmb][l2size] = {}
			for algorithm, runs in algorithms.items():
				speedup_results[csmb][l2size][algorithm] = {}

				for run_name, results in runs.items():
					stp = 0
					hms = 0
					walltime = 0
					mpki = 0

					for benchmark, results in results.items():
						cycles = results['cycles']
						walltime = max(walltime, results['walltime'])
						mpki = max(mpki, results['mpki'])

						hms = hms + cycles/benchmark_results[csmb][l2size]['lru'][benchmark]
						stp = stp + benchmark_results[csmb][l2size]['lru'][benchmark]/cycles if cycles > 0 else 0

					hms = len(results) / hms if hms > 0 else 0

					speedup_results[csmb][l2size][algorithm][run_name] = {
						'stp': stp,
						'hms': hms,
						'walltime': walltime,
						'mpki': mpki
				}
	return speedup_results

# Covert verbose run names to workload group specifiers, results are sorted by their position in the group
def transform_data_group_names(results, aliases):
	renamed_results = {}

	for csmb, runs in results.items():

		renamed_results[csmb] = {}
		for l2size, algorithms in runs.items():
			renamed_results[csmb][l2size] = {}
			for algorithm, runs in algorithms.items():
				renamed_results[csmb][l2size][algorithm] = {}

				for run_name, results in runs.items():
					found = False
					for name, alias, title in aliases:
						i = 0
						for benchmarks in benchmark_sets[name]:
							if '-'.join(benchmarks) == run_name:
								renamed_results[csmb][l2size][algorithm]["%s-%02d" % (alias, i)] = results
								found = True
								break
							i = i + 1

						if found:
							break

				renamed_results[csmb][l2size][algorithm] = collections.OrderedDict(sorted(renamed_results[csmb][l2size][algorithm].items()))

	return renamed_results

# Loads all simulation data from the given run directory, transforms it into stp, hsm and walltime results and renames according to aliases and benchmark.py
def load_results(rundir, aliases):
	benchmark_results, workload_results = extract_raw_data(rundir);	
	return transform_data_group_names(transform_data_to_speedup(benchmark_results, workload_results), aliases);

def plot_results(results, aliases, metrics, algorithms, rundir, yLimit = None, plotname = 'default', dropLRU = False, plotYLine = None, includeLegend = None, includeAverage=True):
	fig,ax = plt.subplots()
	for csmb in results.keys():
		for l2size in results[csmb].keys():

			# Plot for each csmb-l2size combination
			overview = []
			for name, alias, title in aliases:
				# Find number of runs executed in this group
				N = len([key for key,value in results[csmb][l2size]['lru'].items() if key.startswith(alias+"-")])

				if N == 0:
					print "No lru data for group %s (%s, csmb %s l2 %s), skipping." % (title,name, csmb, l2size)
					continue 

				# Create indicies, cacluate bar width
				ind = np.arange(N+1) if includeAverage else np.arange(N)
				width = 1.0/(len(algorithms)+2)

				for metric in metrics:

					if plotYLine != None:
						plt.plot(range(N+2), [plotYLine] * (N+2), ':', color="black")


					# Build one bar plot per algorithm
					rects = []
					i = 1
					values = {}
					count_algorithms = len(algorithms) - 1 if dropLRU else 0
					for algorithm in algorithms:
						if algorithm == "lru" and dropLRU: continue
						values[algorithm] = [value[metric] for key,value in results[csmb][l2size][algorithm].items() if key.startswith(alias+"-")]
						if len(values[algorithm]) != N:
							values[algorithm] = [0] * (N)
						
						if includeAverage:
							values[algorithm].append(np.mean(values[algorithm]))

						rects.append(ax.bar(ind+(i*width), values[algorithm], width*.8, color=cm.Blues(1.*i/count_algorithms), linewidth=0.45))
						i += 1

					ax.set_ylabel("%s" % metric)
					ax.set_xticks(ind+0.5)
					xlabels = [key for key,val in results[csmb][l2size]['lru'].items() if key.startswith(alias+"-")] + (['avg'] if includeAverage else [])
					ax.set_xticklabels(xlabels, rotation=45)
					
					# Generate legends if requested
					if includeLegend != None and includeLegend(alias, metric):
						handles = ()
						legends = ()
						count = 0
						for algorithm in algorithms:
							if algorithm == "lru" and dropLRU: continue
							handles += (rects[count][0],)
							legends += (algorithm,)
							count += 1
						ax.legend(handles, legends, fontsize='small', ncol=2, bbox_to_anchor=(0.5, 1.45), loc='upper center')


					ax.set_ylim(yLimit if yLimit != None else [0.6,1.3])
					ax.set_xlim([0,N+1]) if includeAverage else ax.set_xlim([0,N])

					fig.set_size_inches((2.5+2.5*np.floor(N/5), 2.5))
					plt.savefig('%s/%s-%s-%s-%s-%s.png' % (args.out, plotname, metric, l2size, csmb, alias), frameon=False, bbox_inches='tight')
					plt.cla()


					# Append data to overview
					if len(overview) == 0:
						overview.append(','.join([''] + [algorithm for algorithm in algorithms]))
					for i in xrange(N + (1 if not dropLRU else 0)):
						if i < N:
							row = ['%s-%s-%02d' % (metric, alias, i)]
						else:
							row = ['%s-%s-avg' % (metric, alias)]

						for algorithm in algorithms:
							if algorithm == "lru" and dropLRU: continue
							row += [str(values[algorithm][i])]

						row = ','.join(row)
						overview.append(row)

			# Dump overview to a csv file
			with open('%s/%s-%s-%s.csv' % (args.out, plotname, l2size, csmb), "w") as file:
				file.write('\n'.join(overview))

# Generate HMS, STP and mpki results relative to LRU for a subset of the l2size and csmb configurations. 
def generate_speedup_results(results, l2size_filter = ['0128k'], csmb_filter = ['0100']):
	generated = {}
	for csmb, l2runs in results.items():
		if csmb not in csmb_filter: continue
		generated[csmb] = {}
		for l2size, algorithms in l2runs.items():
			if l2size not in l2size_filter: continue
			generated[csmb][l2size] = {}

			for algorithm, runs in algorithms.items():
				generated[csmb][l2size][algorithm] = {}

				for alias, values in runs.items():
					generated[csmb][l2size][algorithm][alias] = {}
					for metric, value in values.items():
						# Store value relative to lru result
						generated[csmb][l2size][algorithm][alias][metric] = value/results[csmb][l2size]["lru"][alias][metric]

				generated[csmb][l2size][algorithm] = collections.OrderedDict(sorted(generated[csmb][l2size][algorithm].items()))
	return generated


# Generate per workload size avarages for each csmb-l2size combination, NOTE: Input is the output from generate_speedup_results
def generate_size_averages(results):
	generated = {}
	# Generate the special avg-4, avg-8 and avg-16 runs
	for csmb, l2runs in results.items():
		generated[csmb] = {}
		for l2size, algorithms in l2runs.items():
			generated[csmb][l2size] = {}
			result = {}
			for algorithm, runs in algorithms.items():
				generated[csmb][l2size][algorithm] = {}

				averages = {key: {'hms': [], 'stp': [], 'mpki': []} for key in ['4', '8', '16']}
				fouraverages = {}
				for workload,values in runs.items():
					group = '4'
					if workload.startswith("ra8"): group = '8'
					elif workload.startswith("ra16"): group = '16'
					workload_group = workload.split('-')[0]
					
					if group == '4' and workload_group not in fouraverages:
						fouraverages[workload_group] = {'hms': [], 'stp': [], 'mpki': []};

					for metric in averages[group].keys():
						averages[group][metric].append(values[metric])
						if group == '4':
							fouraverages[workload_group][metric].append(values[metric])

				for cores in averages:
					vals = {}
					for metric in averages[cores]:
						vals[metric] = np.mean(averages[cores][metric]) if len(averages[cores][metric]) > 0 else 0
					generated[csmb][l2size][algorithm]['avg-%02d' % int(cores)] = vals
				for workload_group in fouraverages:
					vals = {}
					for metric in fouraverages[workload_group]:
						vals[metric] = np.mean(fouraverages[workload_group][metric]) if len(fouraverages[workload_group][metric]) > 0 else 0
					generated[csmb][l2size][algorithm]['4-avg-%s' % workload_group] = vals


				generated[csmb][l2size][algorithm] = collections.OrderedDict(sorted(generated[csmb][l2size][algorithm].items()))
	return generated

# Generate relative hms, stp and walltime comparisons accross csmb configurations
def generate_csmb_comparisons(results):
	generated = {}
	# Default csmb value and sorted list of all values in run
	default = '0100';
	csmbs = sorted([int(val) for val in results.keys()])
	generated[default] = {}
	for csmb in csmbs:
		csmb = '%04d' % csmb
		runs = results[csmb]

		for l2size, algorithms in runs.items():
			if l2size not in generated[default]: generated[default][l2size] = {}
			result = {}
			for algorithm, runs in algorithms.items():
				if algorithm not in generated[default][l2size]: generated[default][l2size][algorithm] = collections.OrderedDict()

				averages = {key: {'hms': [], 'stp': [], 'walltime': []} for key in ['4', '8', '16']}

				for workload,values in runs.items():
					group = '4'
					if workload.startswith("ra8"): group = '8'
					elif workload.startswith("ra16"): group = '16'
					
					for metric in averages[group].keys():
						averages[group][metric].append(values[metric]/results[default][l2size][algorithm][workload][metric])

				
				# Reduce list to averages and update generated
				for cores in averages:
					vals = {}
					for metric in averages[cores]:
						vals[metric] = np.mean(averages[cores][metric]) if len(averages[cores][metric]) > 0 else 0
					generated[default][l2size][algorithm]['csmb-%s-%s' % (cores, csmb)] = vals

	return generated

parser = argparse.ArgumentParser(description='Workload speedup/throughput grapher')
parser.add_argument('--dir', default='.', help='Experiment run directory')
parser.add_argument('--out', default='_figures/speedup/', help='Figure output directory')

args = parser.parse_args()

if not path.exists(args.dir):
	exit('Run directory %s does not exist!' % args.dir)

if not path.exists(args.out):
	os.makedirs(args.out)
	print 'Created output directory %s' % args.out

rundir = path.realpath(args.dir)

# Mapping between workload names in benchmarks.py (ex workload-cache) and the report name (ex ca4)
aliases = [
	('workload-cache', 'ca4', 'Cache'),
	('workload-cache-bw', 'cabw4', 'Cache-Bandwidth'),
	('workload-bw', 'bw4', 'Bandwidth'),
	('workload-compute', 'co4', 'Compute'),
	('workload-random', 'ra4', 'Random'),
	('workload-8', 'ra8', 'Random 8'),
	('workload-16', 'ra16', 'Random 16'),
]




# Partition algorithms to plot
algorithms = ['lru', 'tadip', 'drrip-3', 'ucp', 'pipp', 'pipp-min8', 'prism']

#for algorithm in algorithms_names:
#	for csmb in speedup_results.keys():
#		for l2size in speedup_results[csmb].keys():
#			# A incomplete run will skrew complete runs with different l2 size, stupid, but the easiest way of acheiving stability
#			if algorithm not in speedup_results[csmb][l2size]:				
#				print "Missing results for algorithm %s (l2 %s), removing this algorithm from plot." % (algorithm, l2size)
#				algorithms_names.remove(algorithm)


results = load_results(rundir, aliases);

# Plot speedup result and average stp and hms plots for base experiment
legend = lambda alias, metric: alias.startswith('ra')
speedup = generate_speedup_results(results)
plot_results(speedup, aliases, ['stp', 'hms', 'mpki'], algorithms, rundir, plotname='speedup', dropLRU=True, plotYLine=1, includeLegend=lambda alias, metric: alias.startswith('ra'));
plot_results(generate_size_averages(speedup), [('', 'avg', ''), ('', '4-avg', '')], ['stp', 'hms', 'mpki'], algorithms, rundir, includeAverage=False, plotname='avg', dropLRU=True, plotYLine=1, yLimit=[0.5,1.2], includeLegend=lambda alias, metric: (alias == 'avg' and metric=='mpki') or (alias == '4-avg' and metric=='stp'));
speedup = None

# Plot csmb comparisons
#csmb_results = generate_csmb_comparisons(results) 
#plot_results(csmb_results, [('', 'csmb-4', '')], ['stp', 'hms'], algorithms, rundir, plotname='csmb', yLimit=[0.98,1.02]);
#plot_results(csmb_results, [('', 'csmb-4', '')], ['walltime'], algorithms, rundir, plotname='csmb', yLimit=[0,2]);