#!/usr/bin/env python

import argparse
import os
import numpy as np
from matplotlib import pyplot as plt
from matplotlib import cm
import matplotlib
from scipy.stats import gmean
#matplotlib.rcParams.update({'font.size': 20})

from os import path
from sys import exit
import collections

from build import profiles
from benchmarks import benchmark_sets

def extract_ipc(filename, coreno = 0):
	instructions = extract_property('performance_model.instruction_count', filename, coreno)
	cycles = extract_property('performance_model.cycle_count', filename, coreno)
	if cycles == None: return 0.1
	return instructions/cycles

def extract_cycles(filename, coreno = 0):
	return extract_property('performance_model.cycle_count', filename, coreno)

def extract_property(property, filename, coreno = 0):
	try:
		with open(filename) as f:
			for line in f:
				line = line.split(' = ')
				if line[0] == property:
					return float(line[1].split(',')[coreno])
	except:
		return 0
	return 0

#print profiles

parser = argparse.ArgumentParser(description='Workload speedup/throughput grapher')
parser.add_argument('--dir', default='.', help='Experiment run directory')
parser.add_argument('--out', default='_figures/speedup/', help='Figure output directory')

args = parser.parse_args()

if not path.exists(args.dir):
	exit('Run directory %s does not exist!' % args.dir)

if not path.exists(args.out):
	os.makedirs(args.out)
	print 'Created output directory %s' % args.out

rundir = path.realpath(args.dir)

benchmark_results = {}
workload_results = {}

for subdir, dirs, files in os.walk(rundir):
	for name in dirs:
		run = name.split('.')
		run_name = run[0]

		# Filter out invalid folders
		if run_name.startswith('_') or len(run) == 1:
			continue;

		benchmarks = run_name.split('-')
		algorithm = run[2]
		l2size = [key.split('-')[1] for key in run if key.startswith("l2-")][0]
		if l2size not in benchmark_results: benchmark_results[l2size] = {}
		if l2size not in workload_results: workload_results[l2size] = {}


		if len(benchmarks) == 1:
			# use only 4M l3 configurations as baseline
			if not "l3-04M" in name: continue

			if algorithm not in benchmark_results[l2size]: benchmark_results[l2size][algorithm] = {}
			benchmark_results[l2size][algorithm][run_name] = extract_cycles('%s/%s/stats-benchmark-0.txt' % (rundir, name))
		else:
			if algorithm not in workload_results[l2size]: workload_results[l2size][algorithm] = {}
			results = {}
			i = 0
			for benchmark in benchmarks:
				results[benchmark] = extract_cycles('%s/%s/stats-benchmark-%d.txt' % (rundir, name, i), i)
				i = i + 1
			workload_results[l2size][algorithm][run_name] = results

# Transform workload results to STP and HMS measurements
speedup_results = {}
for l2size, algorithms in workload_results.items():
	speedup_results[l2size] = {}
	for algorithm, runs in algorithms.items():
		speedup_results[l2size][algorithm] = {}

		for run_name, results in runs.items():
			stp = 0
			hms = 0

			for benchmark, cycles in results.items():
				hms = hms + cycles/benchmark_results[l2size]['lru'][benchmark]
				stp = stp + benchmark_results[l2size]['lru'][benchmark]/cycles if cycles > 0 else 0

			hms = len(results) / hms if hms > 0 else 0

			speedup_results[l2size][algorithm][run_name] = {
				'stp': stp,
				'hms': hms
		}

# convert run names to group names
aliases = [
	('workload-cache', 'ca4', 'Cache'),
	('workload-cache-bw', 'cabw4', 'Cache-Bandwidth'),
	('workload-bw', 'bw4', 'Bandwidth'),
	('workload-compute', 'co4', 'Compute'),
	('workload-random', 'ra4', 'Random'),
	('workload-8', 'ra8', 'Random 8'),
	('workload-16', 'ra16', 'Random 16'),
]
# Partition algorithms to plot
algorithms_names = ['lru', 'tadip', 'drrip-3', 'ucp', 'pipp', 'pipp-min8', 'prism']
for algorithm in algorithms_names:
	for l2size in speedup_results.keys():
		# A incomplete run will skrew complete runs with different l2 size, stupid, but the easiest way of acheiving stability
		if algorithm not in speedup_results[l2size]:				
			print "Missing results for algorithm %s (l2 %s), removing this algorithm from plot." % (algorithm, l2size)
			algorithms_names.remove(algorithm)

renamed_results = {}

for l2size, algorithms in speedup_results.items():
	renamed_results[l2size] = {}
	for algorithm, runs in algorithms.items():
		renamed_results[l2size][algorithm] = {}

		for run_name, results in runs.items():
			found = False
			for name, alias, title in aliases:
				i = 0
				for benchmarks in benchmark_sets[name]:
					if '-'.join(benchmarks) == run_name:
						renamed_results[l2size][algorithm]["%s-%02d" % (alias, i)] = results
						found = True
						break
					i = i + 1

				if found:
					break

		renamed_results[l2size][algorithm] = collections.OrderedDict(sorted(renamed_results[l2size][algorithm].items()))

for l2size in renamed_results.keys():
	overview = []
	for name, alias, title in aliases:
		# Find number of runs executed in this group
		N = len([key for key,value in renamed_results[l2size]['lru'].items() if key.startswith(alias+"-")])

		if N == 0:
			print "No lru data for group %s (%s, l2 %s), skipping." % (title,name, l2size)
			continue 

		# Create indicies, cacluate bar width
		ind = np.arange(N+1)
		width = 1.0/(len(algorithms_names)+2)

		for metric in ['stp', 'hms']:
			fig,ax = plt.subplots()

			# Build one bar plot per algorithm
			rects = []
			i = 1
			values = {}
			for algorithm in algorithms_names:
				values[algorithm] = [value[metric] for key,value in renamed_results[l2size][algorithm].items() if key.startswith(alias+"-")]
				if len(values[algorithm]) != N:
					values[algorithm] = [0] * (N+1)
				else:
					values[algorithm].append(gmean(values[algorithm]))

				rects.append(ax.bar(ind+(i*width), values[algorithm], width*.8, color=cm.Blues(1.*i/len(algorithms_names)), linewidth=0.45))
				i += 1

			ax.set_ylabel("%s" % metric)
			ax.set_xticks(ind+0.5)
			ax.set_xticklabels([ key for key,val in renamed_results[l2size]['lru'].items() if key.startswith(alias+"-")] + ['avg'], rotation=45)
			
			# Generate legends, but only for the ra images
			if alias == "ra4" or (metric == "stp" and alias.startswith("ra")):
				handles = ()
				legends = ()
				count = 0
				for algorithm in algorithms_names:
					handles += (rects[count][0],)
					legends += (algorithm,)
					count += 1
				ax.legend(handles, legends, fontsize='small', ncol=3, bbox_to_anchor=(0.5, 1.40), loc='upper center')
			

			if metric == 'stp': 
				if alias == 'ra8':
					ax.set_ylim([2,8])		
				elif alias == 'ra16':
					ax.set_ylim([2,16])		
				else:
					ax.set_ylim([1,4])		
			if metric == 'hms': ax.set_ylim([0,1])
			ax.set_xlim([0,N+1])

			fig.set_size_inches((2.5+2.5*np.floor(N/5), 2.5))
			plt.savefig('%s/%s-%s-%s.png' % (args.out, metric, l2size, alias), frameon=False, bbox_inches='tight')
			plt.clf()


			# Append data to overview
			if len(overview) == 0:
				overview.append(','.join([''] + [algorithm + ',' for algorithm in algorithms_names]))
			for i in xrange(N+1):
				if i < N:
					row = ['%s-%s-%02d' % (metric, alias, i)]
				else:
					row = ['%s-%s-avg' % (metric, alias)]

				for algorithm in algorithms_names:
					if values['lru'][i] > 0:
						row += [str(values[algorithm][i]), str((values[algorithm][i]-values['lru'][i])/values['lru'][i])]
					else:
						row += [str(values[algorithm][i]),'0']

				row = ','.join(row)
				overview.append(row)

	# Dump overview to a csv file
	with open('%s/%s.csv' % (args.out, l2size), "w") as file:
		file.write('\n'.join(overview))